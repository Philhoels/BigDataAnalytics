---
title: "BDA2 - Spark SQL - Exercises"
author: "Andreas Christopoulos Charitos (andch552) & Phillip HÃ¶lscher (phiho267)"
date: "11 5 2019"
output: 
  pdf_document:
    toc: True
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage



# BDA2 - Spark - Exercises

*Every .txt output will be attached as separate file.* 

In this set of exercises you will work exclusively with Spark. This means that in your programs, you only need to create the **SparkContext** .

In a number of exercises you will be asked to calculated temperature averages (daily and monthly). These are not always computed according to the standard definition of average. In this domain the daily average temperature is calculated by averaging the daily measured maximum and the daily measured minimum temperatures. The monthly average is calculated by averaging the daily maximums and minimums for that month. For example, to get the monthly average for October, take maximums and minimums for each day, sum them up and divide by 62 (which is the same as taking the daily averages, summing them up and divide by the number of days).

There are two ways to write queries in Spark SQL - using built-in API functions or running SQL-like queries. *To pass this lab, you need to*:
- (1) use built-in API functions for all the questions
- (2) write a regular SQL query for the second question

For each question include the following data in the report and sort it as shown:

- 1. year, station with the max, maxValue ORDER BY maxValue DESC year, station with the min, minValue ORDER BY minValue DESC
- 2. year, month, value ORDER BY value DESC year, month, value ORDER BY value DESC
- 3. year, month, station, avgMonthlyTemperature ORDER BY avgMonthlyTemperature DESC
- 4. station, maxTemp, maxDailyPrecipitation ORDER BY station DESC
Note: The correct result for this question should be empty. 
- 5. year, month, avgMonthlyPrecipitation ORDER BY year DESC, month DESC
- 6. year, month, difference ORDER BY year DESC, month DESC

## Assignments

## Assignments 1
- 1) What are the lowest and highest temperatures measured each year for the period 1950-2014. Provide the lists sorted in the descending order with respect to the maximum temperature. In this exercise you will use the **temperature-readings.csv**  file.

## Assignments 1a
- a) Extend the program to include the station number (*not the station name*) where the maximum/minimum temperature wasmeasured.

Code:
```{r, eval=FALSE}
# here come the code for assigment 1a - max
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row  
from pyspark.sql import functions as F
sc =SparkContext()
sqlContext=SQLContext(sc)

temps= sc.textFile("/user/x_phiho/data/temperature-readings.csv")
parts = temps.map(lambda l:l.split(";"))
tempReadings = parts.map(lambda p: Row(station=p[0],  date=p[1], year=p[1].split("-")[0], time=p[2],  Temp=float(p[3]), quality=p[4]))
#Inferring the schema and registering the DataFrame as atable
schemaTempReadings =  sqlContext.createDataFrame(tempReadings)
schemaTempReadings.registerTempTable("tempReadings")

max_temps=schemaTempReadings.select(["year","station","Temp"]).filter((schemaTempReadings['year'] <=2014) & (schemaTempReadings["year"]>=1950 ))\
.groupby(['year','station']).agg(F.max('Temp').alias("max_temp")).orderBy('max_temp',ascending=False)

min_temps=schemaTempReadings.select(["year","station","Temp"]).filter((schemaTempReadings['year'] <=2014) & (schemaTempReadings["year"]>=1950 ))\
.groupby(['year','station']).agg(F.min('Temp').alias("min_temp")).orderBy('min_temp',ascending=False)

max_temps_rdd = max_temps.rdd
min_temps_rdd = min_temps.rdd

max_temps_rdd.saveAsTextFile("results/BDA2_1_max")
min_temps_rdd.saveAsTextFile("results/BDA2_1_min")

```

Output - max:
```{r, eval=FALSE}
# here comes the output - the frist rows
Row(year=u'1975', station=u'86200', max_temp=36.1)
Row(year=u'1975', station=u'95160', max_temp=35.8)
Row(year=u'1975', station=u'96550', max_temp=35.6)
Row(year=u'1975', station=u'106100', max_temp=35.5)
Row(year=u'1992', station=u'63600', max_temp=35.4)
Row(year=u'1975', station=u'75240', max_temp=35.4)
Row(year=u'1992', station=u'63050', max_temp=35.2)
Row(year=u'1975', station=u'96350', max_temp=35.0)
Row(year=u'1975', station=u'96030', max_temp=35.0)
Row(year=u'1992', station=u'85040', max_temp=35.0)
Row(year=u'1992', station=u'76000', max_temp=35.0)
Row(year=u'1975', station=u'97390', max_temp=35.0)
Row(year=u'1992', station=u'75240', max_temp=35.0)


```


Output:
```{r, eval=FALSE}
# here comes the output - the last rows
Row(year=u'1999', station=u'181900', min_temp=-47.0)
Row(year=u'1966', station=u'192830', min_temp=-47.0)
Row(year=u'1987', station=u'123480', min_temp=-47.3)
Row(year=u'1966', station=u'181900', min_temp=-47.5)
Row(year=u'1978', station=u'155940', min_temp=-47.7)
Row(year=u'1966', station=u'189780', min_temp=-48.0)
Row(year=u'1999', station=u'191910', min_temp=-48.7)
Row(year=u'1999', station=u'191720', min_temp=-48.8)
Row(year=u'1999', station=u'192830', min_temp=-49.0)
Row(year=u'1966', station=u'166870', min_temp=-49.3)
Row(year=u'1966', station=u'179950', min_temp=-49.4)
```

\newpage
## Assignments 2
- 2) Count the number of readings for each month in the period of 1950-2014 which are higher than 10 degrees. Repeat the exercise, this time taking only distinct readings from each station. That is, if a station reported a reading above 10 degrees in some month, then it appears only once in the count for that month.
In this exercise you will use the **temperature-readings.csv** file. The output should contain the following information:
Year, month, count

Code:
```{r, eval=FALSE}
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row  
from pyspark.sql import functions as F
sc =SparkContext()
sqlContext=SQLContext(sc)


temp=sc.textFile("/user/x_phiho/data/temperature-readings.csv")
parts=temp.map(lambda a: a.split(';'))
tempReadings=parts.map(lambda x: Row(station=x[0], date=x[1], year=int(x[1].split("-")[0]), month=int(x[1].split("-")[1]), time=x[2], value=float(x[3]), quality=x[4]))
schemaTempReadings=sqlContext.createDataFrame(tempReadings)

schemaTempReadings.registerTempTable("tempReadingsTable")

df_sql=sqlContext.sql('SELECT year, month, count(station) as value FROM tempReadingsTable WHERE year>=1950 and year<=2014 and value>10.0 group by year, month ORDER BY value DESC').show()

df_sql_distinct=sqlContext.sql('SELECT year, month, count(distinct station) as value FROM tempReadingsTable WHERE year>=1950 and year<=2014 and value>10.0 group by year, month ORDER BY value DESC').show()

df_sql=df_sql.rdd
df_sql_distinct=df_sql_distinct.rdd

df_sql.saveAsTextFile("results/BDA2_2SQL_results2")
df_sql_distinct.saveAsTextFile("results/BDA2_2SQL_distinct_results2")
```

We had a problem to save the results as rdd and used the function "show()" to print the outout of the script. Below you can see the output of df_sql_distinct show().
Output:
```{r, eval=FALSE}
  File "BDA2_2SQL_combined.py", line 19, in <module>
    df_sql=df_sql.rdd
AttributeError: 'NoneType' object has no attribute 'rdd'

+----+-----+------+
|year|month| value|
+----+-----+------+
|2014|    7|147681|
|2011|    7|146656|
|2010|    7|143419|
|2012|    7|137477|
|2013|    7|133657|
|2009|    7|133008|
|2011|    8|132734|
|2009|    8|128349|
|2013|    8|128235|
|2003|    7|128133|
|2002|    7|127956|
|2006|    8|127622|
|2008|    7|126973|
|2002|    8|126073|
|2005|    7|125294|
|2011|    6|125193|
|2012|    8|125037|
|2006|    7|124794|
|2010|    8|124417|
|2014|    8|124045|
+----+-----+------+
only showing top 20 rows

+----+-----+-----+
|year|month|value|
+----+-----+-----+
|1972|   10|  378|
|1973|    6|  377|
|1973|    5|  377|
|1973|    9|  376|
|1972|    8|  376|
|1972|    9|  375|
|1972|    6|  375|
|1971|    8|  375|
|1972|    5|  375|
|1972|    7|  374|
|1971|    6|  374|
|1971|    9|  374|
|1971|    5|  373|
|1973|    8|  373|
|1974|    8|  372|
|1974|    6|  372|
|1970|    8|  370|
|1974|    9|  370|
|1973|    7|  370|
|1974|    5|  370|
+----+-----+-----+
only showing top 20 rows

```

\newpage
## Assignments 3
- 3) Find the average monthly temperature for each available station in Sweden. Your result should include average temperature for each station for each month in the period of 1960- 2014. Bear in mind that not every station has the readings for each month in this timeframe. In this exercise you will use the *temperature-readings.csv* file.
The output should contain the following information:
Year, month, station number, average monthly temperature

Code:
```{r, eval=FALSE}
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row  
from pyspark.sql import functions as F
sc =SparkContext()
sqlContext=SQLContext(sc)

temps= sc.textFile("/user/x_phiho/data/temperature-readings.csv")
parts = temps.map(lambda l:l.split(";"))
tempReadings = parts.map(lambda p: Row(station=p[0],  date=p[1], year=p[1].split("-")[0],month=p[1].split("-")[1],day=p[1].split("-")[2], time=p[2],  Temp=float(p[3]), quality=p[4]))
#Inferring the schema and registering the DataFrame as atable
schemaTempReadings =  sqlContext.createDataFrame(tempReadings)
schemaTempReadings.registerTempTable("tempReadings")


schemaAvg=schemaTempReadings.select('year','month','day','station','Temp')\
.filter((schemaTempReadings['year'] <=2014) & (schemaTempReadings["year"]>=1960 )).groupby( "year","month",'day','station')\
.agg( (F.avg('Temp') ).alias("daily_avg_temp"))\
.select('year','month','station','daily_avg_temp').groupBy('year','month','station')\
.agg( (F.avg('daily_avg_temp') ).alias("avg_temperature"))\
.orderBy('year','month','station',ascending=False)

schemaAvg_rdd = schemaAvg.rdd

schemaAvg_rdd.saveAsTextFile("results/BDA2_3_results")
```

Output:
```{r, eval=FALSE}
# results of the first rows
Row(year=u'2014', month=u'12', station=u'99450', avg_temperature=1.9897849462365587)
Row(year=u'2014', month=u'12', station=u'99280', avg_temperature=2.3321236559139775)
Row(year=u'2014', month=u'12', station=u'99270', avg_temperature=2.434301075268817)
Row(year=u'2014', month=u'12', station=u'98490', avg_temperature=-1.0755376344086023)
Row(year=u'2014', month=u'12', station=u'98290', avg_temperature=0.5602150537634408)
Row(year=u'2014', month=u'12', station=u'98230', avg_temperature=0.4267473118279569)
Row(year=u'2014', month=u'12', station=u'98210', avg_temperature=0.6279569892473118)
Row(year=u'2014', month=u'12', station=u'98180', avg_temperature=0.13225806451612887)
Row(year=u'2014', month=u'12', station=u'98040', avg_temperature=0.5327956989247311)
Row(year=u'2014', month=u'12', station=u'97530', avg_temperature=-1.6580837173579106)
Row(year=u'2014', month=u'12', station=u'97510', avg_temperature=-1.1068548387096777)
Row(year=u'2014', month=u'12', station=u'97400', avg_temperature=-0.7415322580645163)
Row(year=u'2014', month=u'12', station=u'97370', avg_temperature=-1.4209167410429686)
```

\newpage
## Assignments 4
- 4) Provide a list of stations with their associated maximum measured temperatures and maximum measured daily precipitation. Show only those stations where the maximum temperature is between 25 and 30 degrees and maximum daily precipitation is between 100 mm and 200mm.
In this exercise you will use the **temperature-readings.csv** and **precipitation-readings.csv** files.
The output should contain the following information:
Station number, maximum measured temperature, maximum daily precipitation

Code:
```{r, eval=FALSE}
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row  
from pyspark.sql import functions as F
sc =SparkContext()
sqlContext=SQLContext(sc)

temps= sc.textFile("/user/x_phiho/data/temperature-readings.csv")
parts_temp = temps.map(lambda l:l.split(";"))
tempReadings = parts_temp.map(lambda p: Row(station=p[0],  date=p[1], year=p[1].split("-")[0],month=p[1].split("-")[1], time=p[2],  Temp=float(p[3]), quality=p[4]))
#Inferring the schema and registering the DataFrame as atable
schemaTempReadings =  sqlContext.createDataFrame(tempReadings)
schemaTempReadings.registerTempTable("tempReadings")


precipitation = sc.textFile("/user/x_phiho/data/precipitation-readings.csv")

parts_precip=precipitation.map(lambda a: a.split(';'))
precReadingsRow=parts_precip.map(lambda x: Row(station=x[0], date=x[1], year=int(x[1].split("-")[0]), month=int(x[1].split("-")[1]), time=x[2], precip=float(x[3]), quality=x[4]))
precReadingsRow=sqlContext.createDataFrame(precReadingsRow)
precReadingsRow.registerTempTable("precipReadings")


sch=schemaTempReadings.groupby('station').agg(F.max('Temp').alias('maxTemp'))
sch1=sch.filter( (sch['maxTemp'] >25) & (sch['maxTemp'] <30) )

prec=precReadingsRow.groupby('station').agg(F.max('precip').alias('maxPrecip'))
prec1=prec.filter( (prec['maxPrecip'] >100) & (prec['maxPrecip'] <200) )

joinedSchema=sch1.join(prec1,'station')

joinedSchema_rdd = joinedSchema.rdd
joinedSchema_rdd.saveAsTextFile("results/BDA2_4_results")

```

Output:
```{r, eval=FALSE}

```

The results rdd (joinedSchema_rdd) is empty. 

\newpage
## Assignments 5
- 5) Calculate the average monthly precipitation for the ??stergotland region (list of stations is provided in the separate file) for the period 1993-2016. In order to do this, you will first need to calculate the total monthly precipitation for each station before calculating the monthly average (by averaging over stations).
In this exercise you will use the **precipitation-readings.csv** and **stations-Ostergotland.csv** files. HINT (not for the SparkSQL lab): Avoid using joins here! stations-Ostergotland.csv is small and if distributed will cause a number of unnecessary shuffles when joined with precipitationRDD. If you distribute **precipitation-readings.csv** then either repartition your stations RDD to 1 partition or make use of the collect to acquire a python list and broadcast function to broadcast the list to all nodes.
The output should contain the following information: Year, month, average monthly precipitation

Code:
```{r, eval=FALSE}
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row  
from pyspark.sql import functions as F
sc =SparkContext()
sqlContext=SQLContext(sc)


precipitation = sc.textFile("/user/x_phiho/data/precipitation-readings.csv")

parts=precipitation.map(lambda a: a.split(';'))
precReadingsRow=parts.map(lambda x: Row(station=x[0], date=x[1], year=int(x[1].split("-")[0]), month=int(x[1].split("-")[1]), time=x[2], precip=float(x[3]), quality=x[4]))
schemaPrecReadings=sqlContext.createDataFrame(precReadingsRow)

rdd = sc.textFile("/user/x_phiho/data/stations-Ostergotland.csv")

parts=rdd.map(lambda a: a.split(';'))
statOstRow=parts.map(lambda x: Row(station=x[0], name=x[1]))
schemaStatOst=sqlContext.createDataFrame(statOstRow)

schemaPrecReadings=schemaPrecReadings.filter( (schemaPrecReadings['year'] >= 1993) & (schemaPrecReadings['year'] <= 2016) )

schemaPrecReadings=schemaPrecReadings.join(schemaStatOst, 'station', 'inner')

schemaPrecReadings=schemaPrecReadings.groupBy('station', 'year', 'month').agg(F.sum('precip')).groupBy('year', 'month').agg(F.avg('sum(precip)').alias('avg_monthly_precipitation')).orderBy(['year', 'month'], ascending=[0, 0]).show()

schemaPrecReadings_rdd = schemaPrecReadings.rdd

schemaPrecReadings_rdd.saveAsTextFile("results/BDA2_5_results")
```

Output:
```{r, eval=FALSE}
  File "BDA2_5.py", line 26, in <module>
    schemaPrecReadings_rdd = schemaPrecReadings.rdd
AttributeError: 'NoneType' object has no attribute 'rdd'

+----+-----+-------------------------+
|year|month|avg_monthly_precipitation|
+----+-----+-------------------------+
|2016|    7|                      0.0|
|2016|    6|                  47.6625|
|2016|    5|       29.250000000000004|
|2016|    4|        26.90000000000001|
|2016|    3|       19.962500000000002|
|2016|    2|                  21.5625|
|2016|    1|                   22.325|
|2015|   12|       28.924999999999997|
|2015|   11|        63.88750000000002|
|2015|   10|                   2.2625|
|2015|    9|       101.29999999999998|
|2015|    8|       26.987499999999997|
|2015|    7|       119.09999999999997|
|2015|    6|        78.66250000000002|
|2015|    5|        93.22499999999998|
|2015|    4|       15.337499999999999|
|2015|    3|        42.61250000000001|
|2015|    2|       24.824999999999996|
|2015|    1|        59.11250000000003|
|2014|   12|        35.46250000000001|
+----+-----+-------------------------+
only showing top 20 rows
```


\newpage
## Assignments 6
- 6) Compare the average monthly temperature (find the difference) in the period 1950-2014 for all stations in ??stergotland with long-term monthly averages in the period of 1950-1980. Make a plot of your results.
HINT: The first step is to find the monthly averages for each station. Then, you can average over all stations to acquire the average temperature for a specific year and month. This RDD/Data Frame can be used to compute the long-term average by averaging over all the years in theinterval.
The output should contain the following information: Year, month, difference
Code:

```{r, eval=FALSE}
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row  
from pyspark.sql import functions as F
sc =SparkContext()
sqlContext=SQLContext(sc)

temp=sc.textFile("/user/x_phiho/data/precipitation-readings.csv")

parts_temp=temp.map(lambda a: a.split(';'))
tempReadingsRow=parts_temp.map(lambda x: Row(station=x[0], date=x[1], year=int(x[1].split("-")[0]),month= int(x[1].split("-")[1]),time= x[2],Temp= float(x[3]),quality= x[4]))
schemaTempReadings=sqlContext.createDataFrame(tempReadingsRow)


precip = sc.textFile("/user/x_phiho/data/stations-Ostergotland.csv")

parts_precip=precip.map(lambda a: a.split(';'))
statOstRow=parts_precip.map(lambda x: Row(station=x[0], name=x[1]))
schemaStatOst=sqlContext.createDataFrame(statOstRow)

schemaTempReadings=schemaTempReadings.join(schemaStatOst, 'station', 'inner')

schemaTempReadings=schemaTempReadings.filter( (schemaTempReadings['year'] >= 1950) & (schemaTempReadings['year'] <= 2014) )

minMaxTemps=schemaTempReadings.groupby(['station', 'date', 'year', 'month']).agg(F.min('Temp'), F.max('Temp'))

avgMonthlyTemps=minMaxTemps.withColumn( 'dailyAvg', (minMaxTemps['min(Temp)']+minMaxTemps['max(Temp)'])/2.0 ).groupBy('year', 'month', 'station').agg(F.avg('dailyAvg').alias('avgMonthlyTempPerStat')).groupBy('year', 'month').agg(F.avg('avgMonthlyTempPerStat').alias('avgMonthlyTemperature'))

longTermAvgTemp=avgMonthlyTemps.filter(avgMonthlyTemps['year'] <= 1980).groupBy('month').agg(F.avg('avgMonthlyTemperature').alias('longTermAvgTemp'))

diff=avgMonthlyTemps.join(longTermAvgTemp, 'month', 'inner')
diff=diff.withColumn('difference', diff['avgMonthlyTemperature']-diff['longTermAvgTemp']).select('year', 'month', 'difference').orderBy(['year', 'month'], ascending=[0, 0]).show()

diff_rdd = diff.rdd

diff_rdd.saveAsTextFile("results/BDA2_6_results")
```

Output:
```{r, eval=FALSE}
 File "BDA2_6.py", line 33, in <module>
    diff_rdd = diff.rdd
AttributeError: 'NoneType' object has no attribute 'rdd'
+----+-----+----------+
|year|month|difference|
+----+-----+----------+
+----+-----+----------+
```




