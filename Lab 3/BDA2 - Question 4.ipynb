{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Assignemnt 4__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Provide a list of stations with their associated maximum measured temperatures and maximum measured daily precipitation. Show only those stations where the maximum temperature is between 25 and 30 degrees and maximum daily precipitation is between 100 mm and 200 mm. In this exercise you will use the *temperature-readings.csv* and *precipitation-readings.csv* files.\n",
    "\n",
    "The output should contain the following information:\n",
    "__Station number, maximum measured temperature, maximum daily precipitation __\n",
    "\n",
    "__HINT__: \n",
    "The correct result for this question should be empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each question include the following data in the report and sort it as shown:\n",
    "    \n",
    "__station, maxTemp, maxDailyPrecipitation ORDER BY station DESC__\n",
    "\n",
    "__Note__: The correct result for this question should be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the setup\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row  \n",
    "from pyspark.sql import functions as F\n",
    "sc =SparkContext()\n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to temperature-readings-tiny.csv\n",
    "path_unifolder_temp_tiny = \"/Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/temperature-readings-tiny.csv\"\n",
    "path_unifolder_precip_tiny = \"/Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/precipitation-readings-tiny.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Temp=6.8, date='2013-11-01', month='11', quality='G', station='102170', time='06:00:00', year='2013'),\n",
       " Row(Temp=3.8, date='2013-11-01', month='11', quality='G', station='102170', time='18:00:00', year='2013'),\n",
       " Row(Temp=5.8, date='2013-11-02', month='11', quality='G', station='102170', time='06:00:00', year='2013'),\n",
       " Row(Temp=-1.1, date='2013-11-02', month='11', quality='G', station='102170', time='18:00:00', year='2013'),\n",
       " Row(Temp=-0.2, date='2013-11-03', month='11', quality='G', station='102170', time='06:00:00', year='2013'),\n",
       " Row(Temp=5.6, date='2013-11-03', month='11', quality='G', station='102170', time='18:00:00', year='2013'),\n",
       " Row(Temp=6.5, date='2013-11-04', month='11', quality='G', station='102170', time='06:00:00', year='2013'),\n",
       " Row(Temp=5.1, date='2013-11-04', month='11', quality='G', station='102170', time='18:00:00', year='2013'),\n",
       " Row(Temp=4.2, date='2013-11-05', month='11', quality='G', station='102170', time='06:00:00', year='2013'),\n",
       " Row(Temp=3.2, date='2013-11-05', month='11', quality='G', station='102170', time='18:00:00', year='2013')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and create rdd, this needs to be tranfsormed to data frame later on\n",
    "temps = sc.textFile(path_unifolder_temp_tiny)\n",
    "parts_temps = temps.map(lambda l:l.split(\";\"))\n",
    "tempReadings = parts_temps.map(lambda p: Row(station=p[0],  date=p[1], year=p[1].split(\"-\")[0], month=p[1].split(\"-\")[1], time=p[2],  Temp=float(p[3]), quality=p[4]))\n",
    "tempReadings.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Precip=0.0, date='1995-08-01', day='01', month='08', quality='Y', station='103100', time='00:00:00', year='1995'),\n",
       " Row(Precip=0.0, date='1995-08-01', day='01', month='08', quality='Y', station='103100', time='01:00:00', year='1995'),\n",
       " Row(Precip=0.0, date='1995-08-01', day='01', month='08', quality='Y', station='103100', time='02:00:00', year='1995'),\n",
       " Row(Precip=0.0, date='1995-08-01', day='01', month='08', quality='Y', station='103100', time='03:00:00', year='1995'),\n",
       " Row(Precip=0.0, date='1995-08-01', day='01', month='08', quality='Y', station='103100', time='04:00:00', year='1995'),\n",
       " Row(Precip=0.0, date='1995-08-01', day='01', month='08', quality='Y', station='103100', time='05:00:00', year='1995'),\n",
       " Row(Precip=0.0, date='1995-08-01', day='01', month='08', quality='Y', station='103100', time='06:00:00', year='1995'),\n",
       " Row(Precip=0.0, date='1995-08-01', day='01', month='08', quality='Y', station='103100', time='07:00:00', year='1995'),\n",
       " Row(Precip=0.0, date='1995-08-01', day='01', month='08', quality='Y', station='103100', time='08:00:00', year='1995'),\n",
       " Row(Precip=0.0, date='1995-08-01', day='01', month='08', quality='Y', station='103100', time='09:00:00', year='1995')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and create rdd, this needs to be tranfsormed to data frame later on\n",
    "precip = sc.textFile(path_unifolder_precip_tiny)\n",
    "parts_precip = precip.map(lambda l:l.split(\";\"))\n",
    "precipReadings = parts_precip.map(lambda p: Row(station=p[0],  date=p[1], year=p[1].split(\"-\")[0], month=p[1].split(\"-\")[1], day=p[1].split(\"-\")[2], time=p[2],  Precip=float(p[3]), quality=p[4]))\n",
    "precipReadings.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+-------+-------+--------+----+\n",
      "|Temp|      date|month|quality|station|    time|year|\n",
      "+----+----------+-----+-------+-------+--------+----+\n",
      "| 6.8|2013-11-01|   11|      G| 102170|06:00:00|2013|\n",
      "| 3.8|2013-11-01|   11|      G| 102170|18:00:00|2013|\n",
      "| 5.8|2013-11-02|   11|      G| 102170|06:00:00|2013|\n",
      "|-1.1|2013-11-02|   11|      G| 102170|18:00:00|2013|\n",
      "|-0.2|2013-11-03|   11|      G| 102170|06:00:00|2013|\n",
      "| 5.6|2013-11-03|   11|      G| 102170|18:00:00|2013|\n",
      "| 6.5|2013-11-04|   11|      G| 102170|06:00:00|2013|\n",
      "| 5.1|2013-11-04|   11|      G| 102170|18:00:00|2013|\n",
      "| 4.2|2013-11-05|   11|      G| 102170|06:00:00|2013|\n",
      "| 3.2|2013-11-05|   11|      G| 102170|18:00:00|2013|\n",
      "+----+----------+-----+-------+-------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a data frame from rdd\n",
    "schemaTempReadings =  sqlContext.createDataFrame(tempReadings)\n",
    "schemaTempReadings.registerTempTable(\"tempReadings\")\n",
    "schemaTempReadings.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-----+-------+-------+--------+----+\n",
      "|Precip|      date|day|month|quality|station|    time|year|\n",
      "+------+----------+---+-----+-------+-------+--------+----+\n",
      "|   0.0|1995-08-01| 01|   08|      Y| 103100|00:00:00|1995|\n",
      "|   0.0|1995-08-01| 01|   08|      Y| 103100|01:00:00|1995|\n",
      "|   0.0|1995-08-01| 01|   08|      Y| 103100|02:00:00|1995|\n",
      "|   0.0|1995-08-01| 01|   08|      Y| 103100|03:00:00|1995|\n",
      "|   0.0|1995-08-01| 01|   08|      Y| 103100|04:00:00|1995|\n",
      "|   0.0|1995-08-01| 01|   08|      Y| 103100|05:00:00|1995|\n",
      "|   0.0|1995-08-01| 01|   08|      Y| 103100|06:00:00|1995|\n",
      "|   0.0|1995-08-01| 01|   08|      Y| 103100|07:00:00|1995|\n",
      "|   0.0|1995-08-01| 01|   08|      Y| 103100|08:00:00|1995|\n",
      "|   0.0|1995-08-01| 01|   08|      Y| 103100|09:00:00|1995|\n",
      "+------+----------+---+-----+-------+-------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a data frame from rdd\n",
    "schemaPrecipReadings =  sqlContext.createDataFrame(precipReadings)\n",
    "schemaPrecipReadings.registerTempTable(\"precipReadings\")\n",
    "schemaPrecipReadings.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ The temp case:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|station|Temp_max|\n",
      "+-------+--------+\n",
      "| 102170|    29.1|\n",
      "| 102190|    29.0|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the max temp for each station\n",
    "schemaTempReadings_max = schemaTempReadings\\\n",
    ".select([\"station\", \"Temp\"])\\\n",
    ".groupBy(\"station\").agg(F.max(\"Temp\").alias(\"Temp_max\"))\\\n",
    ".orderBy(\"Temp_max\", ascending = False)\n",
    "\n",
    "schemaTempReadings_max.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|station|Temp_max|\n",
      "+-------+--------+\n",
      "| 102170|    29.1|\n",
      "| 102190|    29.0|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaTempReadings_max_filterd = schemaTempReadings_max.select([\"station\", \"Temp_max\"]).filter((schemaTempReadings_max[\"Temp_max\"] >= 25) & \n",
    "                                                                                              (schemaTempReadings_max[\"Temp_max\"] <=30))\n",
    "schemaTempReadings_max_filterd.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ The precip case:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+---+------+\n",
      "|station|year|month|day|Precip|\n",
      "+-------+----+-----+---+------+\n",
      "| 103100|1995|   08| 01|   0.0|\n",
      "| 103100|1995|   08| 01|   0.0|\n",
      "| 103100|1995|   08| 01|   0.0|\n",
      "| 103100|1995|   08| 01|   0.0|\n",
      "| 103100|1995|   08| 01|   0.0|\n",
      "| 103100|1995|   08| 01|   0.0|\n",
      "| 103100|1995|   08| 01|   0.0|\n",
      "| 103100|1995|   08| 01|   0.0|\n",
      "| 103100|1995|   08| 01|   0.0|\n",
      "| 103100|1995|   08| 01|   0.0|\n",
      "+-------+----+-----+---+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take just the variables we need\n",
    "schemaPrecipReadings = schemaPrecipReadings\\\n",
    ".select([\"station\", \"year\", \"month\", \"day\", \"Precip\"])\n",
    "\n",
    "schemaPrecipReadings.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+---+----------------+\n",
      "|station|year|month|day|Precip_daily_sum|\n",
      "+-------+----+-----+---+----------------+\n",
      "| 103100|1996|   10| 15|             0.0|\n",
      "| 103100|1996|   10| 14|             1.6|\n",
      "| 103100|1996|   10| 13|             5.0|\n",
      "| 103100|1996|   10| 12|            11.3|\n",
      "| 103100|1996|   10| 11|             0.2|\n",
      "| 103100|1996|   10| 10|             0.0|\n",
      "| 103100|1996|   10| 09|             0.0|\n",
      "| 103100|1996|   10| 08|             0.7|\n",
      "| 103100|1996|   10| 07|             0.1|\n",
      "| 103100|1996|   10| 06|             0.0|\n",
      "+-------+----+-----+---+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute the max measured daily precip\n",
    "# we need to group by year, month, day and sum up all Precip for this condition\n",
    "schemaPrecipReadings_sum = schemaPrecipReadings\\\n",
    ".groupBy(\"station\", \"year\", \"month\", \"day\")\\\n",
    ".agg(F.sum(\"Precip\").alias(\"Precip_daily_sum\"))\\\n",
    ".orderBy([\"station\", \"year\", \"month\", \"day\"], ascending = False)\n",
    "\n",
    "schemaPrecipReadings_sum.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+\n",
      "|station|maxDailyPrecipitation|\n",
      "+-------+---------------------+\n",
      "| 103100|                 21.6|\n",
      "+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the max for each station\n",
    "schemaPrecipReadings_station_max = schemaPrecipReadings_sum\\\n",
    ".select([\"station\", \"Precip_daily_sum\"])\\\n",
    ".groupBy(\"station\").agg(F.max(\"Precip_daily_sum\").alias(\"maxDailyPrecipitation\"))\\\n",
    ".orderBy([\"station\"], ascending = False)\n",
    "\n",
    "schemaPrecipReadings_station_max.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+\n",
      "|station|maxDailyPrecipitation|\n",
      "+-------+---------------------+\n",
      "+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter for precip restrictions\n",
    "schemaPrecipReadings_station_max_filter = schemaPrecipReadings_station_max\\\n",
    ".filter((schemaPrecipReadings_station_max[\"maxDailyPrecipitation\"] > 100) &\n",
    "       (schemaPrecipReadings_station_max[\"maxDailyPrecipitation\"] < 200))\n",
    "\n",
    "schemaPrecipReadings_station_max_filter.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------------+\n",
      "|station|Temp_max|maxDailyPrecipitation|\n",
      "+-------+--------+---------------------+\n",
      "+-------+--------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now we have to join the two data frames together\n",
    "joined_df = schemaTempReadings_max_filterd.join(schemaPrecipReadings_station_max_filter, 'station', 'inner')\\\n",
    ".orderBy('station', ascending=False)\n",
    "\n",
    "joined_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
