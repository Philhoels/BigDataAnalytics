{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Working path\"\n",
    "#path /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/temperature-readings.csv\" 64728769L, 2146781232C\u001b[?2004l\n",
      "\"~/Desktop/temperature-readings.csv\" 64728769L, 2146781232C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/precipitation-readings.csv\" 20292769L, 661060228C\u001b[?2004l\n",
      "\"~/Desktop/precipitation-readings.csv\" 20292769L, 661060228C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/stations-Ostergotland.csv\" [dos] 34L, 2860C\u001b[?2004l\n",
      "\"~/Desktop/stations-Ostergotland.csv\" [dos] 34L, 2860C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/stations.csv\" 812L, 67697C\u001b[?2004l\n",
      "\"~/Desktop/stations.csv\" 812L, 67697C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8"
     ]
    }
   ],
   "source": [
    "# Remove UTF-8 BOM\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/temperature-readings.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/precipitation-readings.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/stations-Ostergotland.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/stations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIM - Vi IMproved 8.0 (2016 Sep 12, compiled Aug 17 2018 17:24:51)\n",
      "Too many edit arguments: \"-\"\n",
      "More info with: \"vim -h\"\n",
      "VIM - Vi IMproved 8.0 (2016 Sep 12, compiled Aug 17 2018 17:24:51)\n",
      "Too many edit arguments: \"-\"\n",
      "More info with: \"vim -h\"\n",
      "VIM - Vi IMproved 8.0 (2016 Sep 12, compiled Aug 17 2018 17:24:51)\n",
      "Too many edit arguments: \"-\"\n",
      "More info with: \"vim -h\"\n",
      "VIM - Vi IMproved 8.0 (2016 Sep 12, compiled Aug 17 2018 17:24:51)\n",
      "Too many edit arguments: \"-\"\n",
      "More info with: \"vim -h\"\n"
     ]
    }
   ],
   "source": [
    "# Remove UTF-8 BOM\n",
    "'''\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/temperature-readings.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/precipitation-readings.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/stations-Ostergotland.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/stations.csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/temperature-readings-tiny.csv\" 10000L, 336854C\u001b[?2004l\n",
      "\"~/Desktop/temperature-readings-tiny.csv\" 10000L, 336854C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/precipitation-readings-tiny.csv\" 10000L, 330001C\u001b[?2004l\n",
      "\"~/Desktop/precipitation-readings-tiny.csv\" 10000L, 330001C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8"
     ]
    }
   ],
   "source": [
    "# Remove UTF-8 BOM\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/temperature-readings-tiny.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/precipitation-readings-tiny.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove UTF-8 BOM\n",
    "'''\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/temperature-readings-tiny.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/precipitation-readings-tiny.csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext,SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of Spark Context in the PySpark shell is 2.3.1\n",
      "The Python version of Spark Context in the PySpark shell is 3.6\n",
      "The master of Spark Context in the PySpark shell is local[*]\n"
     ]
    }
   ],
   "source": [
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\quartermaine\\\\Documents\\\\Big Data Analytics Labs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp= sc.textFile(\"C:/Users/quartermaine/Documents/Big Data Analytics Labs/temperature-readings-tiny.csv\", minPartitions =5)\n",
    "\n",
    "temp_csv = spark.read.csv(\"temperature-readings-tiny.csv\", header=False,sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------+-----------+-------+\n",
      "|Station_number|      Date|    Time|Temperature|Quality|\n",
      "+--------------+----------+--------+-----------+-------+\n",
      "|        102170|2013-11-01|06:00:00|        6.8|      G|\n",
      "|        102170|2013-11-01|18:00:00|        3.8|      G|\n",
      "|        102170|2013-11-02|06:00:00|        5.8|      G|\n",
      "|        102170|2013-11-02|18:00:00|       -1.1|      G|\n",
      "|        102170|2013-11-03|06:00:00|       -0.2|      G|\n",
      "|        102170|2013-11-03|18:00:00|        5.6|      G|\n",
      "|        102170|2013-11-04|06:00:00|        6.5|      G|\n",
      "|        102170|2013-11-04|18:00:00|        5.1|      G|\n",
      "|        102170|2013-11-05|06:00:00|        4.2|      G|\n",
      "|        102170|2013-11-05|18:00:00|        3.2|      G|\n",
      "|        102170|2013-11-06|06:00:00|        1.7|      G|\n",
      "|        102170|2013-11-06|18:00:00|        0.9|      G|\n",
      "|        102170|2013-11-07|06:00:00|       -0.1|      G|\n",
      "|        102170|2013-11-07|18:00:00|        0.1|      G|\n",
      "|        102170|2013-11-08|06:00:00|       -1.2|      G|\n",
      "|        102170|2013-11-08|18:00:00|        5.3|      G|\n",
      "|        102170|2013-11-09|06:00:00|        5.6|      G|\n",
      "|        102170|2013-11-09|18:00:00|        3.8|      G|\n",
      "|        102170|2013-11-10|06:00:00|        2.2|      G|\n",
      "|        102170|2013-11-10|18:00:00|       -1.7|      G|\n",
      "+--------------+----------+--------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Station_number: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Temperature: string (nullable = true)\n",
      " |-- Quality: string (nullable = true)\n",
      "\n",
      "The type of names_df is <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=temp_csv.selectExpr(\"_c0 as Station_number\", \"_c1 as Date\", \"_c2 as Time\", \"_c3 as Temperature\",\"_c4 as Quality\")\n",
    "df.show()\n",
    "df.printSchema()\n",
    "print(\"The type of names_df is\", type(df))\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------+-----------+-------+\n",
      "|Station_number|      Date|    Time|Temperature|Quality|\n",
      "+--------------+----------+--------+-----------+-------+\n",
      "|        102190|1955-09-01|18:00:00|       15.2|      Y|\n",
      "|        102190|1955-09-01|12:00:00|       19.8|      Y|\n",
      "|        102190|1955-09-01|06:00:00|       14.5|      Y|\n",
      "|        102190|1955-09-02|06:00:00|       11.6|      Y|\n",
      "|        102190|1955-09-02|18:00:00|       12.3|      Y|\n",
      "|        102190|1955-09-02|12:00:00|       15.8|      Y|\n",
      "|        102190|1955-09-03|12:00:00|       14.5|      Y|\n",
      "|        102190|1955-09-03|18:00:00|       13.4|      Y|\n",
      "|        102190|1955-09-03|06:00:00|       11.5|      Y|\n",
      "|        102190|1955-09-04|18:00:00|       12.6|      Y|\n",
      "|        102190|1955-09-04|12:00:00|       18.0|      Y|\n",
      "|        102190|1955-09-04|06:00:00|        7.7|      Y|\n",
      "|        102190|1955-09-05|12:00:00|       17.1|      Y|\n",
      "|        102190|1955-09-05|18:00:00|       12.2|      Y|\n",
      "|        102190|1955-09-05|06:00:00|        7.8|      Y|\n",
      "|        102190|1955-09-06|12:00:00|       18.4|      Y|\n",
      "|        102190|1955-09-06|18:00:00|       17.2|      Y|\n",
      "|        102190|1955-09-06|06:00:00|       13.0|      Y|\n",
      "|        102190|1955-09-07|18:00:00|       12.1|      Y|\n",
      "|        102190|1955-09-07|12:00:00|       20.4|      Y|\n",
      "+--------------+----------+--------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8905"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "tem1=df.filter( (f.year(df.Date)>=1950)).filter((f.year(df.Date)<=2014) )\n",
    "tem1.orderBy(tem1.Date).show()\n",
    "tem1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply(get_stats).unstack()\n",
    "\n",
    "# t=tem1.select(\n",
    "#     year(\"Date\").alias('year'),\n",
    "#     \"Temperature\")\n",
    "# t.groupBy('year').apply(get_stats).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|year|Temperature|\n",
      "+----+-----------+\n",
      "|1955|       14.5|\n",
      "|1955|       12.1|\n",
      "|1955|       19.8|\n",
      "|1955|       15.2|\n",
      "|1955|       11.6|\n",
      "|1955|       15.8|\n",
      "|1955|       12.3|\n",
      "|1955|       11.5|\n",
      "|1955|       14.5|\n",
      "|1955|       13.4|\n",
      "|1955|        7.7|\n",
      "|1955|       18.0|\n",
      "|1955|       12.6|\n",
      "|1955|        7.8|\n",
      "|1955|       17.1|\n",
      "|1955|       12.2|\n",
      "|1955|       13.0|\n",
      "|1955|       18.4|\n",
      "|1955|       17.2|\n",
      "|1955|        7.0|\n",
      "+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf=tem1.select(f.year(\"Date\").alias('year'),\"Temperature\")\n",
    "#tem1.select(f.year(tem1.Date).alias('year'),tem1.Temperature)\n",
    "#newdf = newdf.withColumn('year',\n",
    "#        col('year').cast('int'))\n",
    "newdf.orderBy(newdf.year).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+--------------+\n",
      "|year|maxTemperature|minTemperature|\n",
      "+----+--------------+--------------+\n",
      "|1959|           9.8|          -0.1|\n",
      "|1955|           9.9|          -0.2|\n",
      "|1961|           9.8|          -0.2|\n",
      "|2013|           8.9|          -0.1|\n",
      "|1956|           9.9|          -0.1|\n",
      "|2014|           9.9|          -0.1|\n",
      "|1957|           9.9|          -0.1|\n",
      "|1960|           9.9|          -0.1|\n",
      "|1958|           9.8|          -0.1|\n",
      "+----+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.groupBy(newdf.year).agg(f.max(newdf.Temperature).alias(\"maxTemperature\"),f.min(newdf.Temperature).alias(\"minTemperature\")).show()\n",
    "\n",
    "# dd = newdf.join(\n",
    "#     a,\n",
    "#     on = \"year\",\n",
    "#     how = \"inner\"\n",
    "# )\n",
    "# dd.show()\n",
    "\n",
    "# from pyspark.sql import Window\n",
    "# import pyspark.sql.functions as F\n",
    "# window = Window.partitionBy(\"year\")\n",
    "# a = newdf.withColumn(\n",
    "#     (F.max(F.col(\"year\")).over(window)).alias(\"max\")\n",
    "# )\n",
    "# a.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Table with max temperatures per year---\n",
      "+----+----------------+\n",
      "|year|max(Temperature)|\n",
      "+----+----------------+\n",
      "|1955|             9.9|\n",
      "|1956|             9.9|\n",
      "|1957|             9.9|\n",
      "|1958|             9.8|\n",
      "|1959|             9.8|\n",
      "|1960|             9.9|\n",
      "|1961|             9.8|\n",
      "|2013|             8.9|\n",
      "|2014|             9.9|\n",
      "+----+----------------+\n",
      "\n",
      "----Table with min temperatures per year---\n",
      "+----+----------------+\n",
      "|year|min(Temperature)|\n",
      "+----+----------------+\n",
      "|1955|            -0.2|\n",
      "|1956|            -0.1|\n",
      "|1957|            -0.1|\n",
      "|1958|            -0.1|\n",
      "|1959|            -0.1|\n",
      "|1960|            -0.1|\n",
      "|1961|            -0.2|\n",
      "|2013|            -0.1|\n",
      "|2014|            -0.1|\n",
      "+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----Table with max temperatures per year---\")\n",
    "newdf.groupby('year').agg({'Temperature': 'max'}).orderBy('year').show()\n",
    "print(\"----Table with min temperatures per year---\")\n",
    "newdf.groupby('year').agg({'Temperature': 'min'}).orderBy('year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1)What are the lowest and highest temperatures measured each year for the period 1950-2014. Provide the lists sorted in the descending order with respect to the maximum temperature. In this exercise you will use the temperature-readings.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = temp.map(lambda line: line.split(\";\"))\n",
    "\n",
    "year_temperature = lines.map(lambda x: (x[1][0:4], float(x[3])))\n",
    "\n",
    "year_temperature = year_temperature.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014)\n",
    "\n",
    "max_temperatures= year_temperature.reduceByKey(lambda x,y :x if x>=y else y)\n",
    "\n",
    "max_temperaturesSorted=max_temperatures.sortBy(ascending=False,keyfunc=lambda k:k[1])\n",
    "\n",
    "max_temps=spark.createDataFrame(max_temperaturesSorted, schema=['Year', 'Temp'])\n",
    "\n",
    "min_temperatures= year_temperature.reduceByKey(lambda x,y :x if x<=y else y)\n",
    "\n",
    "min_temperaturesSorted=min_temperatures.sortBy(ascending=False,keyfunc=lambda k:k[1])\n",
    "\n",
    "min_temps=spark.createDataFrame(min_temperaturesSorted, schema=['Year', 'Temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Table with max temperatures per year---\n",
      "+----+----+\n",
      "|Year|Temp|\n",
      "+----+----+\n",
      "|2014|29.1|\n",
      "|1960|29.0|\n",
      "|1959|28.2|\n",
      "|1958|28.1|\n",
      "|1956|26.0|\n",
      "|1957|25.2|\n",
      "|1955|20.4|\n",
      "|1961|19.0|\n",
      "|2013|10.2|\n",
      "+----+----+\n",
      "\n",
      "----Table with min temperatures per year---\n",
      "+----+-----+\n",
      "|Year| Temp|\n",
      "+----+-----+\n",
      "|2013|-13.3|\n",
      "|1957|-19.9|\n",
      "|1959|-23.2|\n",
      "|1961|-23.5|\n",
      "|2014|-24.3|\n",
      "|1955|-26.2|\n",
      "|1958|-27.9|\n",
      "|1960|-28.3|\n",
      "|1956|-30.0|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----Table with max temperatures per year---\")\n",
    "max_temps.show()\n",
    "print(\"----Table with min temperatures per year---\")\n",
    "min_temps.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_temp = lines.map(lambda x: (x[1][0:4],( float(x[3]),x[0])))\n",
    "\n",
    "year_temp = year_temp.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014)\n",
    "\n",
    "#############################\n",
    "max_temperatures1= year_temp.reduceByKey(lambda x,y :x if x>=y else y)\n",
    "\n",
    "max_temperaturesSorted1=max_temperatures1.sortBy(ascending=False,keyfunc=lambda k:k[1][0])\n",
    "\n",
    "t=max_temperaturesSorted1.map(lambda x: (x[0], float(x[1][0]),x[1][1]))\n",
    "\n",
    "max_temps1=spark.createDataFrame(t, schema=['Year', 'Temp','station_number'])\n",
    "\n",
    "#############################\n",
    "\n",
    "min_temperatures1= year_temp.reduceByKey(lambda x,y :x if x<=y else y)\n",
    "\n",
    "min_temperaturesSorted1=min_temperatures1.sortBy(ascending=False,keyfunc=lambda k:k[1][0])\n",
    "\n",
    "t1=min_temperaturesSorted1.map(lambda x: (x[0], float(x[1][0]),x[1][1]))\n",
    "\n",
    "min_temps1=spark.createDataFrame(t1, schema=['Year', 'Temp','station_number'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Table with max temperatures per year---\n",
      "+----+----+--------------+\n",
      "|Year|Temp|station_number|\n",
      "+----+----+--------------+\n",
      "|2014|29.1|        102170|\n",
      "|1960|29.0|        102190|\n",
      "|1959|28.2|        102190|\n",
      "|1958|28.1|        102190|\n",
      "|1956|26.0|        102190|\n",
      "|1957|25.2|        102190|\n",
      "|1955|20.4|        102190|\n",
      "|1961|19.0|        102190|\n",
      "|2013|10.2|        102170|\n",
      "+----+----+--------------+\n",
      "\n",
      "----Table with min temperatures per year---\n",
      "+----+-----+--------------+\n",
      "|Year| Temp|station_number|\n",
      "+----+-----+--------------+\n",
      "|2013|-13.3|        102170|\n",
      "|1957|-19.9|        102190|\n",
      "|1959|-23.2|        102190|\n",
      "|1961|-23.5|        102190|\n",
      "|2014|-24.3|        102170|\n",
      "|1955|-26.2|        102190|\n",
      "|1958|-27.9|        102190|\n",
      "|1960|-28.3|        102190|\n",
      "|1956|-30.0|        102190|\n",
      "+----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----Table with max temperatures per year---\")\n",
    "max_temps1.show()\n",
    "print(\"----Table with min temperatures per year---\")\n",
    "min_temps1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_number</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013-11-01</td>\n",
       "      <td>06:00:00</td>\n",
       "      <td>6.8</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013-11-01</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>3.8</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013-11-02</td>\n",
       "      <td>06:00:00</td>\n",
       "      <td>5.8</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013-11-02</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013-11-03</td>\n",
       "      <td>06:00:00</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_number        date      time  Temperature quality\n",
       "0          102170  2013-11-01  06:00:00          6.8       G\n",
       "1          102170  2013-11-01  18:00:00          3.8       G\n",
       "2          102170  2013-11-02  06:00:00          5.8       G\n",
       "3          102170  2013-11-02  18:00:00         -1.1       G\n",
       "4          102170  2013-11-03  06:00:00         -0.2       G"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dat=pd.read_csv(\"temperature-readings-tiny.csv\",sep=';',header=None)\n",
    "dat.columns = ['station_number', 'date','time','Temperature','quality']\n",
    "print(\"first 5 rows\")\n",
    "dat.head(5)\n",
    "# print(\"column names\")\n",
    "# dat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['station_number', 'date', 'Temperature', 'year'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt=dat[['station_number','date','Temperature']]\n",
    "dt['year'] = pd.DatetimeIndex(dt['date']).year\n",
    "dt.head()\n",
    "dt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_number</th>\n",
       "      <th>year</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013</td>\n",
       "      <td>-1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_number  year  Temperature\n",
       "0          102170  2013          6.8\n",
       "1          102170  2013          3.8\n",
       "2          102170  2013          5.8\n",
       "3          102170  2013         -1.1\n",
       "4          102170  2013         -0.2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT=dt[['station_number','year','Temperature']]\n",
    "DT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Temperature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>-26.2</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>-30.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>-19.9</td>\n",
       "      <td>25.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>-27.9</td>\n",
       "      <td>28.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>-23.2</td>\n",
       "      <td>28.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>-28.3</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>-23.5</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>-13.3</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>-24.3</td>\n",
       "      <td>29.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>-19.6</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>-25.5</td>\n",
       "      <td>25.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Temperature      \n",
       "             min   max\n",
       "year                  \n",
       "1955       -26.2  20.4\n",
       "1956       -30.0  26.0\n",
       "1957       -19.9  25.2\n",
       "1958       -27.9  28.1\n",
       "1959       -23.2  28.2\n",
       "1960       -28.3  29.0\n",
       "1961       -23.5  19.0\n",
       "2013       -13.3  10.2\n",
       "2014       -24.3  29.1\n",
       "2015       -19.6  26.8\n",
       "2016       -25.5  25.8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT.groupby('year').agg({'Temperature': ['min', 'max']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)Count the number of readings for each month in the period of 1950-2014 which are higher than 10 degrees.Repeat the exercise,this time taking only distinct readings from each station. That is, if a station reported are a ding above 10 degrees in some month,then it appears only once in the count for that month.In this exercise you will use the temperature-readings.csvfile. \n",
    "#### The out put should contain the following information:Year, month, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lines.map(lambda x: ( (x[1][0:4],x[1][5:7]),float(x[3]) ))\n",
    "\n",
    "y = y.filter(lambda x: int(x[0][0]) >= 1950 and int(x[0][0]) <= 2014)\n",
    "\n",
    "#############################\n",
    "tempsOver= y.filter(lambda x : x[1] >= 10)\n",
    "\n",
    "e=tempsOver.map(lambda x: (x[0][0], x[0][1],float(x[1]) ))\n",
    "\n",
    "M=spark.createDataFrame(e, schema=['Year', 'Month','Temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|Year|Month|cnt|\n",
      "+----+-----+---+\n",
      "|1955|   09| 55|\n",
      "|1955|   10| 23|\n",
      "|1956|   03|  4|\n",
      "|1956|   04|  7|\n",
      "|1956|   05| 60|\n",
      "|1956|   06| 92|\n",
      "|1956|   07|108|\n",
      "|1956|   08| 84|\n",
      "|1956|   09| 54|\n",
      "|1956|   10| 17|\n",
      "|1957|   03|  1|\n",
      "|1957|   04| 13|\n",
      "|1957|   05| 46|\n",
      "|1957|   06| 72|\n",
      "|1957|   07|109|\n",
      "|1957|   08| 96|\n",
      "|1957|   09| 43|\n",
      "|1957|   10|  9|\n",
      "|1958|   04|  4|\n",
      "|1958|   05| 40|\n",
      "+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "M.groupby(\"Year\", \"Month\").agg(f.count('Temp').alias(\"cnt\")).orderBy('Year','Month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3)Find the average monthly temperature for each available station in Sweden. Your result should include average temperature for each station for each month in the period of 1960-2014. Bear in mind that not every station has the readings for each month in this timeframe.In this exercise you will use the temperature-readings.csvfile. \n",
    "#### Theoutputshouldcontainthefollowinginformation: Year, month, station number, average monthly temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = lines.map(lambda x: ( (x[1][0:4],x[1][5:7],x[0]),float(x[3]) ))\n",
    "\n",
    "yy = yy.filter(lambda x: int(x[0][0]) >= 1960 and int(x[0][0]) <= 2014)\n",
    "\n",
    "#############################\n",
    "\n",
    "ee=yy.map(lambda x: (x[0][2],x[0][0], x[0][1],float(x[1]) ))\n",
    "\n",
    "MM=spark.createDataFrame(ee, schema=['Station_number','Year', 'Month','Temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-----+----+\n",
      "|Station_number|Year|Month|Temp|\n",
      "+--------------+----+-----+----+\n",
      "|        102170|2013|   11| 6.8|\n",
      "|        102170|2013|   11| 3.8|\n",
      "|        102170|2013|   11| 5.8|\n",
      "|        102170|2013|   11|-1.1|\n",
      "|        102170|2013|   11|-0.2|\n",
      "|        102170|2013|   11| 5.6|\n",
      "|        102170|2013|   11| 6.5|\n",
      "|        102170|2013|   11| 5.1|\n",
      "|        102170|2013|   11| 4.2|\n",
      "|        102170|2013|   11| 3.2|\n",
      "|        102170|2013|   11| 1.7|\n",
      "|        102170|2013|   11| 0.9|\n",
      "|        102170|2013|   11|-0.1|\n",
      "|        102170|2013|   11| 0.1|\n",
      "|        102170|2013|   11|-1.2|\n",
      "|        102170|2013|   11| 5.3|\n",
      "|        102170|2013|   11| 5.6|\n",
      "|        102170|2013|   11| 3.8|\n",
      "|        102170|2013|   11| 2.2|\n",
      "|        102170|2013|   11|-1.7|\n",
      "+--------------+----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MM.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----+-------------------+\n",
      "|Month|Station_number|Year|    avg_temperature|\n",
      "+-----+--------------+----+-------------------+\n",
      "|   01|        102170|2014|0.42258064516129035|\n",
      "|   02|        102170|2014|0.22580645161290322|\n",
      "|   03|        102170|2014|0.22903225806451616|\n",
      "|   04|        102170|2014|0.39193548387096777|\n",
      "|   05|        102170|2014| 0.3951612903225806|\n",
      "|   06|        102170|2014| 0.2693548387096774|\n",
      "|   07|        102170|2014|0.28064516129032263|\n",
      "|   08|        102170|2014|0.30483870967741933|\n",
      "|   09|        102170|2014| 0.3209677419354839|\n",
      "|   10|        102170|2014| 0.3096774193548387|\n",
      "|   11|        102170|2013| 0.3790322580645161|\n",
      "|   11|        102170|2014| 0.3225806451612903|\n",
      "|   12|        102170|2014|  0.482258064516129|\n",
      "|   12|        102170|2013|               0.35|\n",
      "|   01|        102190|1960|0.37580645161290316|\n",
      "|   01|        102190|1961|0.42903225806451617|\n",
      "|   02|        102190|1960|0.49838709677419357|\n",
      "|   02|        102190|1961|0.47096774193548385|\n",
      "|   03|        102190|1960|0.44999999999999996|\n",
      "|   03|        102190|1961|0.34516129032258064|\n",
      "+-----+--------------+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MM.groupby( \"Month\",\"Station_number\",\"Year\")\\\n",
    ".agg( ((f.max('Temp')-f.min('Temp'))/62).alias(\"avg_temperature\")).orderBy('Station_number','Month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Provide  a  list  of  stations  with  their  associated  maximum  measured  temperatures  and maximum  measured  daily  precipitation.  Show  only  those  stations  where  the  maximum temperature is between 25 and 30 degrees and maximum daily precipitation is between 100  mm  and  200mm.In  this  exercise  you  will  use  the temperature-readings.csvand precipitation-readings.csvfiles.Theoutputshouldcontainthefollowinginformation: Station number, maximum measured temperature, maximum daily precipitation\n",
    "#### HINT: The correct result for this question should be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = temp.map(lambda line: line.split(\";\"))\n",
    "\n",
    "station_temperature = l.map(lambda x: (x[0], float(x[3])))\n",
    "\n",
    "#station_temperature = station_temperature.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014)\n",
    "\n",
    "max_temp_station= station_temperature.reduceByKey(lambda x,y :x if x>=y else y)\n",
    "\n",
    "max_temp_station=max_temp_station.filter(lambda x: x[1]>=25. and x[1]<=30.)\n",
    "\n",
    "w=max_temp_station.map(lambda x: (x[0],float(x[1]) ))\n",
    "\n",
    "W=spark.createDataFrame(w, schema=['Station_number','Temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+\n",
      "|Station_number|Temp|\n",
      "+--------------+----+\n",
      "|        102170|29.1|\n",
      "|        102190|29.0|\n",
      "+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|Station_number|precipitation|\n",
      "+--------------+-------------+\n",
      "|        103100|         13.4|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep=sc.textFile(\"C:/Users/quartermaine/Documents/Big Data Analytics Labs/precipitation-readings-tiny.csv\")\n",
    "\n",
    "L=prep.map(lambda line: line.split(\";\"))\n",
    "\n",
    "st = L.map(lambda x: (x[0], float(x[3])))\n",
    "\n",
    "max_st= st.reduceByKey(lambda x,y :x if x>=y else y)\n",
    "\n",
    "max_st=max_st.filter(lambda x: x[1]>=10. and x[1]<=20.)\n",
    "\n",
    "wst=max_st.map(lambda x: (x[0],float(x[1]) ))\n",
    "\n",
    "WW=spark.createDataFrame(wst, schema=['Station_number','precipitation'])\n",
    "\n",
    "WW.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newRdd = lines.map(lambda x:(x[0])).join(L.map(lambda x:(x[0],(x[1],x[2]))))\n",
    "# newRdd.map(lambda x:(x[1][0], x[0], x[1][1], x[1][1])).coalesce(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Table with max temperatures and precipitation---\n",
      "+--------------+----+--------------+-------------+\n",
      "|Station_number|Temp|Station_number|precipitation|\n",
      "+--------------+----+--------------+-------------+\n",
      "+--------------+----+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inner_join = W.join(WW, W.Station_number == WW.Station_number)\n",
    "print(\"----Table with max temperatures and precipitation---\")\n",
    "inner_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5)Calculate the average monthly precipitation for the Östergotland region (list of stationsis providedintheseparatefile)for the period1993-2016. In order to do this, you will first need to calculate the total monthly precipitation for each station before calculating the monthly average (by averaging over stations).In this exercise you will  use the precipitation-readings.csvand stations-Ostergotland.csv files. HINT(not for the SparkSQL lab): Avoid using joins here! stations-Ostergotland.csvis small  and if distributed will cause a number of unnecessary shuffles when joined with precipitationRDD.  If you distribute precipitation-readings.csv then  either  repartition  your stations RDD to 1 partition or make use of the collect to acquire a python list and broadcast function to broadcast thel ist to all nodes.\n",
    "#### Theoutput should contain the following information: Year,month,average monthly precipitation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations=sc.textFile(\"C:/Users/quartermaine/Documents/Big Data Analytics Labs/stations-Ostergotland.csv\")\n",
    "precipitation=sc.textFile(\"C:/Users/quartermaine/Documents/Big Data Analytics Labs/precipitation-readings-tiny.csv\")\n",
    "\n",
    "precip_split=precipitation.map(lambda line: line.split(\";\"))\n",
    "stations_split=stations.map(lambda line: line.split(\";\"))\n",
    "\n",
    "ostegotland=stations_split.map(lambda x:x[0])\n",
    "\n",
    "precip_data= precip_split.map(lambda x: (x[0], x[1],float(x[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['85490',\n",
       " '85650',\n",
       " '85390',\n",
       " '85410',\n",
       " '86470',\n",
       " '86420',\n",
       " '86350',\n",
       " '86130',\n",
       " '86090',\n",
       " '85280',\n",
       " '85240',\n",
       " '85210',\n",
       " '85180',\n",
       " '84260',\n",
       " '85630',\n",
       " '85460',\n",
       " '85450',\n",
       " '86440',\n",
       " '86340',\n",
       " '86360',\n",
       " '86370',\n",
       " '86200',\n",
       " '86330',\n",
       " '87140',\n",
       " '87150',\n",
       " '85250',\n",
       " '85270',\n",
       " '85220',\n",
       " '85040',\n",
       " '85050',\n",
       " '75520',\n",
       " '85160',\n",
       " '85600',\n",
       " '85130']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ost_stations=ostegotland.distinct().collect()\n",
    "ost_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('103100', '1995-08-01', 0.0),\n",
       " ('103100', '1995-08-01', 0.0),\n",
       " ('103100', '1995-08-01', 0.0)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precip_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\", line 235, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 409, in dump\n",
      "    self.save(obj)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 751, in save_tuple\n",
      "    save(element)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\", line 378, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\", line 529, in save_function_tuple\n",
      "    save(closure_values)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 781, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 805, in _batch_appends\n",
      "    save(x)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\", line 378, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\", line 529, in save_function_tuple\n",
      "    save(closure_values)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 781, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 808, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\", line 378, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\", line 529, in save_function_tuple\n",
      "    save(closure_values)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 781, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 808, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\", line 372, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\", line 525, in save_function_tuple\n",
      "    save(f_globals)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 852, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\pickle.py\", line 496, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\", line 228, in __getnewargs__\n",
      "    \"It appears that you are attempting to broadcast an RDD or reference an RDD from an \"\n",
      "Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[1;34m(self, obj, name)\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mklass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_list\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_appends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36m_batch_appends\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    804\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m                     \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPPENDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[1;34m(self, obj, name)\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mklass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_list\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_appends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36m_batch_appends\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 808\u001b[1;33m                 \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    809\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPPEND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[1;34m(self, obj, name)\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mklass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_list\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_appends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36m_batch_appends\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 808\u001b[1;33m                 \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    809\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPPEND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[1;34m(self, obj, name)\u001b[0m\n\u001b[0;32m    371\u001b[0m                 or themodule is None):\n\u001b[1;32m--> 372\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[1;31m# save the rest of the func data needed by _fill_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call unbound method with explicit self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    851\u001b[0m                 \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 852\u001b[1;33m                 \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    853\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSETITEM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\pickle.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0mrv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m         raise Exception(\n\u001b[1;32m--> 228\u001b[1;33m             \u001b[1;34m\"It appears that you are attempting to broadcast an RDD or reference an RDD from an \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[1;34m\"action or transformation. RDD transformations and actions can only be invoked by the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[1;32m--> 702\u001b[1;33m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    400\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'__repr__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                             \u001b[1;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    695\u001b[0m     \u001b[1;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[1;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getnewargs__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[1;32m-> 2489\u001b[1;33m                                       self._jrdd_deserializer, profiler)\n\u001b[0m\u001b[0;32m   2490\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\u001b[0;32m   2491\u001b[0m                                              self.preservesPartitioning)\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[0;32m   2420\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"serializer should not be empty\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2422\u001b[1;33m     \u001b[0mpickled_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2423\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[0;32m   2424\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[1;34m(sc, command)\u001b[0m\n\u001b[0;32m   2406\u001b[0m     \u001b[1;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2407\u001b[0m     \u001b[0mser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2408\u001b[1;33m     \u001b[0mpickled_command\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2409\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m<<\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 1M\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2410\u001b[0m         \u001b[1;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, protocol)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m     \u001b[0mcp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m     \u001b[0mcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    247\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "precip_data.filter(lambda x : x[0] in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
