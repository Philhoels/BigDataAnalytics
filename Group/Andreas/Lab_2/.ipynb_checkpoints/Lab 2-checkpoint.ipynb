{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Working path\"\n",
    "#path /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/temperature-readings.csv\" 64728769L, 2146781232C\u001b[?2004l\n",
      "\"~/Desktop/temperature-readings.csv\" 64728769L, 2146781232C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/precipitation-readings.csv\" 20292769L, 661060228C\u001b[?2004l\n",
      "\"~/Desktop/precipitation-readings.csv\" 20292769L, 661060228C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/stations-Ostergotland.csv\" [dos] 34L, 2860C\u001b[?2004l\n",
      "\"~/Desktop/stations-Ostergotland.csv\" [dos] 34L, 2860C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/stations.csv\" 812L, 67697C\u001b[?2004l\n",
      "\"~/Desktop/stations.csv\" 812L, 67697C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8"
     ]
    }
   ],
   "source": [
    "# Remove UTF-8 BOM\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/temperature-readings.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/precipitation-readings.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/stations-Ostergotland.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/stations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIM - Vi IMproved 8.0 (2016 Sep 12, compiled Aug 17 2018 17:24:51)\n",
      "Too many edit arguments: \"-\"\n",
      "More info with: \"vim -h\"\n",
      "VIM - Vi IMproved 8.0 (2016 Sep 12, compiled Aug 17 2018 17:24:51)\n",
      "Too many edit arguments: \"-\"\n",
      "More info with: \"vim -h\"\n",
      "VIM - Vi IMproved 8.0 (2016 Sep 12, compiled Aug 17 2018 17:24:51)\n",
      "Too many edit arguments: \"-\"\n",
      "More info with: \"vim -h\"\n",
      "VIM - Vi IMproved 8.0 (2016 Sep 12, compiled Aug 17 2018 17:24:51)\n",
      "Too many edit arguments: \"-\"\n",
      "More info with: \"vim -h\"\n"
     ]
    }
   ],
   "source": [
    "# Remove UTF-8 BOM\n",
    "'''\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/temperature-readings.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/precipitation-readings.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/stations-Ostergotland.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/stations.csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/temperature-readings-tiny.csv\" 10000L, 336854C\u001b[?2004l\n",
      "\"~/Desktop/temperature-readings-tiny.csv\" 10000L, 336854C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/precipitation-readings-tiny.csv\" 10000L, 330001C\u001b[?2004l\n",
      "\"~/Desktop/precipitation-readings-tiny.csv\" 10000L, 330001C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8"
     ]
    }
   ],
   "source": [
    "# Remove UTF-8 BOM\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/temperature-readings-tiny.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/precipitation-readings-tiny.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove UTF-8 BOM\n",
    "'''\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/temperature-readings-tiny.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/precipitation-readings-tiny.csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext,SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of Spark Context in the PySpark shell is 2.3.1\n",
      "The Python version of Spark Context in the PySpark shell is 3.6\n",
      "The master of Spark Context in the PySpark shell is local[*]\n"
     ]
    }
   ],
   "source": [
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\quartermaine\\\\Documents\\\\Big Data Analytics Labs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp= sc.textFile(\"C:/Users/quartermaine/Documents/Big Data Analytics Labs/temperature-readings-tiny.csv\", minPartitions =5)\n",
    "\n",
    "temp_csv = spark.read.csv(\"temperature-readings-tiny.csv\", header=False,sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------+-----------+-------+\n",
      "|Station_number|      Date|    Time|Temperature|Quality|\n",
      "+--------------+----------+--------+-----------+-------+\n",
      "|        102170|2013-11-01|06:00:00|        6.8|      G|\n",
      "|        102170|2013-11-01|18:00:00|        3.8|      G|\n",
      "|        102170|2013-11-02|06:00:00|        5.8|      G|\n",
      "|        102170|2013-11-02|18:00:00|       -1.1|      G|\n",
      "|        102170|2013-11-03|06:00:00|       -0.2|      G|\n",
      "|        102170|2013-11-03|18:00:00|        5.6|      G|\n",
      "|        102170|2013-11-04|06:00:00|        6.5|      G|\n",
      "|        102170|2013-11-04|18:00:00|        5.1|      G|\n",
      "|        102170|2013-11-05|06:00:00|        4.2|      G|\n",
      "|        102170|2013-11-05|18:00:00|        3.2|      G|\n",
      "|        102170|2013-11-06|06:00:00|        1.7|      G|\n",
      "|        102170|2013-11-06|18:00:00|        0.9|      G|\n",
      "|        102170|2013-11-07|06:00:00|       -0.1|      G|\n",
      "|        102170|2013-11-07|18:00:00|        0.1|      G|\n",
      "|        102170|2013-11-08|06:00:00|       -1.2|      G|\n",
      "|        102170|2013-11-08|18:00:00|        5.3|      G|\n",
      "|        102170|2013-11-09|06:00:00|        5.6|      G|\n",
      "|        102170|2013-11-09|18:00:00|        3.8|      G|\n",
      "|        102170|2013-11-10|06:00:00|        2.2|      G|\n",
      "|        102170|2013-11-10|18:00:00|       -1.7|      G|\n",
      "+--------------+----------+--------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Station_number: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Temperature: string (nullable = true)\n",
      " |-- Quality: string (nullable = true)\n",
      "\n",
      "The type of names_df is <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=temp_csv.selectExpr(\"_c0 as Station_number\", \"_c1 as Date\", \"_c2 as Time\", \"_c3 as Temperature\",\"_c4 as Quality\")\n",
    "df.show()\n",
    "df.printSchema()\n",
    "print(\"The type of names_df is\", type(df))\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------+-----------+-------+\n",
      "|Station_number|      Date|    Time|Temperature|Quality|\n",
      "+--------------+----------+--------+-----------+-------+\n",
      "|        102190|1955-09-01|18:00:00|       15.2|      Y|\n",
      "|        102190|1955-09-01|12:00:00|       19.8|      Y|\n",
      "|        102190|1955-09-01|06:00:00|       14.5|      Y|\n",
      "|        102190|1955-09-02|06:00:00|       11.6|      Y|\n",
      "|        102190|1955-09-02|18:00:00|       12.3|      Y|\n",
      "|        102190|1955-09-02|12:00:00|       15.8|      Y|\n",
      "|        102190|1955-09-03|12:00:00|       14.5|      Y|\n",
      "|        102190|1955-09-03|18:00:00|       13.4|      Y|\n",
      "|        102190|1955-09-03|06:00:00|       11.5|      Y|\n",
      "|        102190|1955-09-04|18:00:00|       12.6|      Y|\n",
      "|        102190|1955-09-04|12:00:00|       18.0|      Y|\n",
      "|        102190|1955-09-04|06:00:00|        7.7|      Y|\n",
      "|        102190|1955-09-05|12:00:00|       17.1|      Y|\n",
      "|        102190|1955-09-05|18:00:00|       12.2|      Y|\n",
      "|        102190|1955-09-05|06:00:00|        7.8|      Y|\n",
      "|        102190|1955-09-06|12:00:00|       18.4|      Y|\n",
      "|        102190|1955-09-06|18:00:00|       17.2|      Y|\n",
      "|        102190|1955-09-06|06:00:00|       13.0|      Y|\n",
      "|        102190|1955-09-07|18:00:00|       12.1|      Y|\n",
      "|        102190|1955-09-07|12:00:00|       20.4|      Y|\n",
      "+--------------+----------+--------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8905"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "tem1=df.filter( (f.year(df.Date)>=1950)).filter((f.year(df.Date)<=2014) )\n",
    "tem1.orderBy(tem1.Date).show()\n",
    "tem1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply(get_stats).unstack()\n",
    "\n",
    "# t=tem1.select(\n",
    "#     year(\"Date\").alias('year'),\n",
    "#     \"Temperature\")\n",
    "# t.groupBy('year').apply(get_stats).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|year|Temperature|\n",
      "+----+-----------+\n",
      "|1955|       14.5|\n",
      "|1955|       12.1|\n",
      "|1955|       19.8|\n",
      "|1955|       15.2|\n",
      "|1955|       11.6|\n",
      "|1955|       15.8|\n",
      "|1955|       12.3|\n",
      "|1955|       11.5|\n",
      "|1955|       14.5|\n",
      "|1955|       13.4|\n",
      "|1955|        7.7|\n",
      "|1955|       18.0|\n",
      "|1955|       12.6|\n",
      "|1955|        7.8|\n",
      "|1955|       17.1|\n",
      "|1955|       12.2|\n",
      "|1955|       13.0|\n",
      "|1955|       18.4|\n",
      "|1955|       17.2|\n",
      "|1955|        7.0|\n",
      "+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf=tem1.select(f.year(\"Date\").alias('year'),\"Temperature\")\n",
    "#tem1.select(f.year(tem1.Date).alias('year'),tem1.Temperature)\n",
    "#newdf = newdf.withColumn('year',\n",
    "#        col('year').cast('int'))\n",
    "newdf.orderBy(newdf.year).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+--------------+\n",
      "|year|maxTemperature|minTemperature|\n",
      "+----+--------------+--------------+\n",
      "|1959|           9.8|          -0.1|\n",
      "|1955|           9.9|          -0.2|\n",
      "|1961|           9.8|          -0.2|\n",
      "|2013|           8.9|          -0.1|\n",
      "|1956|           9.9|          -0.1|\n",
      "|2014|           9.9|          -0.1|\n",
      "|1957|           9.9|          -0.1|\n",
      "|1960|           9.9|          -0.1|\n",
      "|1958|           9.8|          -0.1|\n",
      "+----+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.groupBy(newdf.year).agg(f.max(newdf.Temperature).alias(\"maxTemperature\"),f.min(newdf.Temperature).alias(\"minTemperature\")).show()\n",
    "\n",
    "# dd = newdf.join(\n",
    "#     a,\n",
    "#     on = \"year\",\n",
    "#     how = \"inner\"\n",
    "# )\n",
    "# dd.show()\n",
    "\n",
    "# from pyspark.sql import Window\n",
    "# import pyspark.sql.functions as F\n",
    "# window = Window.partitionBy(\"year\")\n",
    "# a = newdf.withColumn(\n",
    "#     (F.max(F.col(\"year\")).over(window)).alias(\"max\")\n",
    "# )\n",
    "# a.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Table with max temperatures per year---\n",
      "+----+----------------+\n",
      "|year|max(Temperature)|\n",
      "+----+----------------+\n",
      "|1955|             9.9|\n",
      "|1956|             9.9|\n",
      "|1957|             9.9|\n",
      "|1958|             9.8|\n",
      "|1959|             9.8|\n",
      "|1960|             9.9|\n",
      "|1961|             9.8|\n",
      "|2013|             8.9|\n",
      "|2014|             9.9|\n",
      "+----+----------------+\n",
      "\n",
      "----Table with min temperatures per year---\n",
      "+----+----------------+\n",
      "|year|min(Temperature)|\n",
      "+----+----------------+\n",
      "|1955|            -0.2|\n",
      "|1956|            -0.1|\n",
      "|1957|            -0.1|\n",
      "|1958|            -0.1|\n",
      "|1959|            -0.1|\n",
      "|1960|            -0.1|\n",
      "|1961|            -0.2|\n",
      "|2013|            -0.1|\n",
      "|2014|            -0.1|\n",
      "+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----Table with max temperatures per year---\")\n",
    "newdf.groupby('year').agg({'Temperature': 'max'}).orderBy('year').show()\n",
    "print(\"----Table with min temperatures per year---\")\n",
    "newdf.groupby('year').agg({'Temperature': 'min'}).orderBy('year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1)What are the lowest and highest temperatures measured each year for the period 1950-2014. Provide the lists sorted in the descending order with respect to the maximum temperature. In this exercise you will use the temperature-readings.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = temp.map(lambda line: line.split(\";\"))\n",
    "\n",
    "year_temperature = lines.map(lambda x: (x[1][0:4], float(x[3])))\n",
    "\n",
    "year_temperature = year_temperature.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014)\n",
    "\n",
    "max_temperatures= year_temperature.reduceByKey(lambda x,y :x if x>=y else y)\n",
    "\n",
    "max_temperaturesSorted=max_temperatures.sortBy(ascending=False,keyfunc=lambda k:k[1])\n",
    "\n",
    "max_temps=spark.createDataFrame(max_temperaturesSorted, schema=['Year', 'Temp'])\n",
    "\n",
    "min_temperatures= year_temperature.reduceByKey(lambda x,y :x if x<=y else y)\n",
    "\n",
    "min_temperaturesSorted=min_temperatures.sortBy(ascending=False,keyfunc=lambda k:k[1])\n",
    "\n",
    "min_temps=spark.createDataFrame(min_temperaturesSorted, schema=['Year', 'Temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Table with max temperatures per year---\n",
      "+----+----+\n",
      "|Year|Temp|\n",
      "+----+----+\n",
      "|2014|29.1|\n",
      "|1960|29.0|\n",
      "|1959|28.2|\n",
      "|1958|28.1|\n",
      "|1956|26.0|\n",
      "|1957|25.2|\n",
      "|1955|20.4|\n",
      "|1961|19.0|\n",
      "|2013|10.2|\n",
      "+----+----+\n",
      "\n",
      "----Table with min temperatures per year---\n",
      "+----+-----+\n",
      "|Year| Temp|\n",
      "+----+-----+\n",
      "|2013|-13.3|\n",
      "|1957|-19.9|\n",
      "|1959|-23.2|\n",
      "|1961|-23.5|\n",
      "|2014|-24.3|\n",
      "|1955|-26.2|\n",
      "|1958|-27.9|\n",
      "|1960|-28.3|\n",
      "|1956|-30.0|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----Table with max temperatures per year---\")\n",
    "max_temps.show()\n",
    "print(\"----Table with min temperatures per year---\")\n",
    "min_temps.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_temp = lines.map(lambda x: (x[1][0:4],( float(x[3]),x[0])))\n",
    "\n",
    "year_temp = year_temp.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014)\n",
    "\n",
    "#############################\n",
    "max_temperatures1= year_temp.reduceByKey(lambda x,y :x if x>=y else y)\n",
    "\n",
    "max_temperaturesSorted1=max_temperatures1.sortBy(ascending=False,keyfunc=lambda k:k[1][0])\n",
    "\n",
    "t=max_temperaturesSorted1.map(lambda x: (x[0], float(x[1][0]),x[1][1]))\n",
    "\n",
    "max_temps1=spark.createDataFrame(t, schema=['Year', 'Temp','station_number'])\n",
    "\n",
    "#############################\n",
    "\n",
    "min_temperatures1= year_temp.reduceByKey(lambda x,y :x if x<=y else y)\n",
    "\n",
    "min_temperaturesSorted1=min_temperatures1.sortBy(ascending=False,keyfunc=lambda k:k[1][0])\n",
    "\n",
    "t1=min_temperaturesSorted1.map(lambda x: (x[0], float(x[1][0]),x[1][1]))\n",
    "\n",
    "min_temps1=spark.createDataFrame(t1, schema=['Year', 'Temp','station_number'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Table with max temperatures per year---\n",
      "+----+----+--------------+\n",
      "|Year|Temp|station_number|\n",
      "+----+----+--------------+\n",
      "|2014|29.1|        102170|\n",
      "|1960|29.0|        102190|\n",
      "|1959|28.2|        102190|\n",
      "|1958|28.1|        102190|\n",
      "|1956|26.0|        102190|\n",
      "|1957|25.2|        102190|\n",
      "|1955|20.4|        102190|\n",
      "|1961|19.0|        102190|\n",
      "|2013|10.2|        102170|\n",
      "+----+----+--------------+\n",
      "\n",
      "----Table with min temperatures per year---\n",
      "+----+-----+--------------+\n",
      "|Year| Temp|station_number|\n",
      "+----+-----+--------------+\n",
      "|2013|-13.3|        102170|\n",
      "|1957|-19.9|        102190|\n",
      "|1959|-23.2|        102190|\n",
      "|1961|-23.5|        102190|\n",
      "|2014|-24.3|        102170|\n",
      "|1955|-26.2|        102190|\n",
      "|1958|-27.9|        102190|\n",
      "|1960|-28.3|        102190|\n",
      "|1956|-30.0|        102190|\n",
      "+----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----Table with max temperatures per year---\")\n",
    "max_temps1.show()\n",
    "print(\"----Table with min temperatures per year---\")\n",
    "min_temps1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_number</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013-11-01</td>\n",
       "      <td>06:00:00</td>\n",
       "      <td>6.8</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013-11-01</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>3.8</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013-11-02</td>\n",
       "      <td>06:00:00</td>\n",
       "      <td>5.8</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013-11-02</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013-11-03</td>\n",
       "      <td>06:00:00</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_number        date      time  Temperature quality\n",
       "0          102170  2013-11-01  06:00:00          6.8       G\n",
       "1          102170  2013-11-01  18:00:00          3.8       G\n",
       "2          102170  2013-11-02  06:00:00          5.8       G\n",
       "3          102170  2013-11-02  18:00:00         -1.1       G\n",
       "4          102170  2013-11-03  06:00:00         -0.2       G"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dat=pd.read_csv(\"temperature-readings-tiny.csv\",sep=';',header=None)\n",
    "dat.columns = ['station_number', 'date','time','Temperature','quality']\n",
    "print(\"first 5 rows\")\n",
    "dat.head(5)\n",
    "# print(\"column names\")\n",
    "# dat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quartermaine\\Anaconda2\\envs\\python3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['station_number', 'date', 'Temperature', 'year'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt=dat[['station_number','date','Temperature']]\n",
    "dt['year'] = pd.DatetimeIndex(dt['date']).year\n",
    "dt.head()\n",
    "dt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_number</th>\n",
       "      <th>year</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013</td>\n",
       "      <td>-1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102170</td>\n",
       "      <td>2013</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_number  year  Temperature\n",
       "0          102170  2013          6.8\n",
       "1          102170  2013          3.8\n",
       "2          102170  2013          5.8\n",
       "3          102170  2013         -1.1\n",
       "4          102170  2013         -0.2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT=dt[['station_number','year','Temperature']]\n",
    "DT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Temperature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>-26.2</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>-30.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>-19.9</td>\n",
       "      <td>25.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>-27.9</td>\n",
       "      <td>28.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>-23.2</td>\n",
       "      <td>28.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>-28.3</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>-23.5</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>-13.3</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>-24.3</td>\n",
       "      <td>29.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>-19.6</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>-25.5</td>\n",
       "      <td>25.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Temperature      \n",
       "             min   max\n",
       "year                  \n",
       "1955       -26.2  20.4\n",
       "1956       -30.0  26.0\n",
       "1957       -19.9  25.2\n",
       "1958       -27.9  28.1\n",
       "1959       -23.2  28.2\n",
       "1960       -28.3  29.0\n",
       "1961       -23.5  19.0\n",
       "2013       -13.3  10.2\n",
       "2014       -24.3  29.1\n",
       "2015       -19.6  26.8\n",
       "2016       -25.5  25.8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT.groupby('year').agg({'Temperature': ['min', 'max']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)Count the number of readings for each month in the period of 1950-2014 which are higher than 10 degrees.Repeat the exercise,this time taking only distinct readings from each station. That is, if a station reported are a ding above 10 degrees in some month,then it appears only once in the count for that month.In this exercise you will use the temperature-readings.csvfile. \n",
    "#### The out put should contain the following information:Year, month, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lines.map(lambda x: ( (x[1][0:4],x[1][5:7]),float(x[3]) ))\n",
    "\n",
    "y = y.filter(lambda x: int(x[0][0]) >= 1950 and int(x[0][0]) <= 2014)\n",
    "\n",
    "#############################\n",
    "tempsOver= y.filter(lambda x : x[1] >= 10)\n",
    "\n",
    "e=tempsOver.map(lambda x: (x[0][0], x[0][1],float(x[1]) ))\n",
    "\n",
    "M=spark.createDataFrame(e, schema=['Year', 'Month','Temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|Year|Month|cnt|\n",
      "+----+-----+---+\n",
      "|1955|   09| 55|\n",
      "|1955|   10| 23|\n",
      "|1956|   03|  4|\n",
      "|1956|   04|  7|\n",
      "|1956|   05| 60|\n",
      "|1956|   06| 92|\n",
      "|1956|   07|108|\n",
      "|1956|   08| 84|\n",
      "|1956|   09| 54|\n",
      "|1956|   10| 17|\n",
      "|1957|   03|  1|\n",
      "|1957|   04| 13|\n",
      "|1957|   05| 46|\n",
      "|1957|   06| 72|\n",
      "|1957|   07|109|\n",
      "|1957|   08| 96|\n",
      "|1957|   09| 43|\n",
      "|1957|   10|  9|\n",
      "|1958|   04|  4|\n",
      "|1958|   05| 40|\n",
      "+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "M.groupby(\"Year\", \"Month\").agg(f.count('Temp').alias(\"cnt\")).orderBy('Year','Month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3)Find the average monthly temperature for each available station in Sweden. Your result should include average temperature for each station for each month in the period of 1960-2014. Bear in mind that not every station has the readings for each month in this timeframe.In this exercise you will use the temperature-readings.csvfile. \n",
    "#### Theoutputshouldcontainthefollowinginformation: Year, month, station number, average monthly temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = lines.map(lambda x: ( (x[1][0:4],x[1][5:7],x[0]),float(x[3]) ))\n",
    "\n",
    "yy = yy.filter(lambda x: int(x[0][0]) >= 1960 and int(x[0][0]) <= 2014)\n",
    "\n",
    "#############################\n",
    "\n",
    "ee=yy.map(lambda x: (x[0][2],x[0][0], x[0][1],float(x[1]) ))\n",
    "\n",
    "MM=spark.createDataFrame(ee, schema=['Station_number','Year', 'Month','Temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-----+----+\n",
      "|Station_number|Year|Month|Temp|\n",
      "+--------------+----+-----+----+\n",
      "|        102170|2013|   11| 6.8|\n",
      "|        102170|2013|   11| 3.8|\n",
      "|        102170|2013|   11| 5.8|\n",
      "|        102170|2013|   11|-1.1|\n",
      "|        102170|2013|   11|-0.2|\n",
      "|        102170|2013|   11| 5.6|\n",
      "|        102170|2013|   11| 6.5|\n",
      "|        102170|2013|   11| 5.1|\n",
      "|        102170|2013|   11| 4.2|\n",
      "|        102170|2013|   11| 3.2|\n",
      "|        102170|2013|   11| 1.7|\n",
      "|        102170|2013|   11| 0.9|\n",
      "|        102170|2013|   11|-0.1|\n",
      "|        102170|2013|   11| 0.1|\n",
      "|        102170|2013|   11|-1.2|\n",
      "|        102170|2013|   11| 5.3|\n",
      "|        102170|2013|   11| 5.6|\n",
      "|        102170|2013|   11| 3.8|\n",
      "|        102170|2013|   11| 2.2|\n",
      "|        102170|2013|   11|-1.7|\n",
      "+--------------+----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MM.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----+-------------------+\n",
      "|Month|Station_number|Year|                avg|\n",
      "+-----+--------------+----+-------------------+\n",
      "|   01|        102170|2014|0.42258064516129035|\n",
      "|   02|        102170|2014|0.22580645161290322|\n",
      "|   03|        102170|2014|0.22903225806451616|\n",
      "|   04|        102170|2014|0.39193548387096777|\n",
      "|   05|        102170|2014| 0.3951612903225806|\n",
      "|   06|        102170|2014| 0.2693548387096774|\n",
      "|   07|        102170|2014|0.28064516129032263|\n",
      "|   08|        102170|2014|0.30483870967741933|\n",
      "|   09|        102170|2014| 0.3209677419354839|\n",
      "|   10|        102170|2014| 0.3096774193548387|\n",
      "|   11|        102170|2013| 0.3790322580645161|\n",
      "|   11|        102170|2014| 0.3225806451612903|\n",
      "|   12|        102170|2013|               0.35|\n",
      "|   12|        102170|2014|  0.482258064516129|\n",
      "|   01|        102190|1960|0.37580645161290316|\n",
      "|   01|        102190|1961|0.42903225806451617|\n",
      "|   02|        102190|1960|0.49838709677419357|\n",
      "|   02|        102190|1961|0.47096774193548385|\n",
      "|   03|        102190|1961|0.34516129032258064|\n",
      "|   03|        102190|1960|0.44999999999999996|\n",
      "+-----+--------------+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MM.groupby( \"Month\",\"Station_number\",\"Year\")\\\n",
    ".agg( ((f.max('Temp')-f.min('Temp'))/62).alias(\"avg\")).orderBy('Station_number','Month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Provide  a  list  of  stations  with  their  associated  maximum  measured  temperatures  and maximum  measured  daily  precipitation.  Show  only  those  stations  where  the  maximum temperatureisbetween25and30degreesandmaximumdailyprecipitationisbetween100  mm  and  200mm.In  this  exercise  you  will  use  the temperature-readings.csvand precipitation-readings.csvfiles.Theoutputshouldcontainthefollowinginformation: Station number, maximum measured temperature, maximum daily precipitation\n",
    "#### HINT: The correct result for this question should be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = temp.map(lambda line: line.split(\";\"))\n",
    "\n",
    "station_temperature = l.map(lambda x: (x[0], float(x[3])))\n",
    "\n",
    "#station_temperature = station_temperature.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014)\n",
    "\n",
    "max_temp_station= station_temperature.reduceByKey(lambda x,y :x if x>=y else y)\n",
    "\n",
    "max_temp_station=max_temp_station.filter(lambda x: x[1]>=25. and x[1]<=30.)\n",
    "\n",
    "w=max_temp_station.map(lambda x: (x[0],float(x[1]) ))\n",
    "\n",
    "W=spark.createDataFrame(w, schema=['Station_number','Temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+\n",
      "|Station_number|Temp|\n",
      "+--------------+----+\n",
      "|        102170|29.1|\n",
      "|        102190|29.0|\n",
      "+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "RDD is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-659b68ad992b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mwst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_st\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mWW\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Station_number'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'precipitation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mWW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    382\u001b[0m         \"\"\"\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \"\"\"\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1394\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1395\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1396\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RDD is empty\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misEmpty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: RDD is empty"
     ]
    }
   ],
   "source": [
    "prep=sc.textFile(\"precipitation-readings-tiny.csv\")\n",
    "\n",
    "L=prep.map(lambda line: line.split(\";\"))\n",
    "\n",
    "st = L.map(lambda x: (x[0], float(x[3])))\n",
    "\n",
    "#station_temperature = station_temperature.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014)\n",
    "\n",
    "max_st= st.reduceByKey(lambda x,y :x if x>=y else y)\n",
    "\n",
    "max_st=max_st.filter(lambda x: x[1]>=0.1 and x[1]<=0.2)\n",
    "\n",
    "wst=max_st.map(lambda x: (x[0],float(x[1]) ))\n",
    "\n",
    "WW=spark.createDataFrame(wst, schema=['Station_number','precipitation'])\n",
    "\n",
    "WW.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newRdd = lines.map(lambda x:(x[0])).join(L.map(lambda x:(x[0],(x[1],x[2]))))\n",
    "# newRdd.map(lambda x:(x[1][0], x[0], x[1][1], x[1][1])).coalesce(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = l.map(lambda x: ( (x[1],x[0]),x[2], x[3]) )\n",
    "rdd2 = L.map(lambda x: ( (x[1],x[0]),x[2],x[3]) )\n",
    "\n",
    "rdd_join = rdd1.join(rdd2)\n",
    "rdd_join.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
