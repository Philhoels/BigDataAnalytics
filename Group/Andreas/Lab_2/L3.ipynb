{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.context import SparkContext,SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)What are the lowest and highest temperatures measured each year for the period 1950-2014. Provide the lists sorted in the descending order with respect to the maximum temperature. In this exercise you will use the temperature-readings.csv file.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------+-----------+-------+\n",
      "|Station_number|      Date|    Time|Temperature|Quality|\n",
      "+--------------+----------+--------+-----------+-------+\n",
      "|        102170|2016-06-11|06:00:00|        9.9|      G|\n",
      "|        102190|1955-09-15|12:00:00|        9.9|      Y|\n",
      "|        102190|1957-05-02|12:00:00|        9.9|      Y|\n",
      "|        102190|1956-09-12|00:00:00|        9.9|      Y|\n",
      "|        102190|1957-06-23|12:00:00|        9.9|      Y|\n",
      "|        102190|1960-06-17|06:00:00|        9.9|      Y|\n",
      "|        102170|2014-09-21|18:00:00|        9.9|      G|\n",
      "|        102170|2015-05-26|18:00:00|        9.9|      G|\n",
      "|        102170|2014-10-08|06:00:00|        9.9|      G|\n",
      "|        102170|2015-06-17|06:00:00|        9.9|      G|\n",
      "|        102190|1956-08-25|00:00:00|        9.9|      Y|\n",
      "|        102190|1956-09-05|00:00:00|        9.9|      Y|\n",
      "|        102170|2015-06-18|06:00:00|        9.9|      G|\n",
      "|        102190|1956-06-16|12:00:00|        9.8|      Y|\n",
      "|        102190|1955-09-16|06:00:00|        9.8|      Y|\n",
      "|        102190|1956-06-28|18:00:00|        9.8|      Y|\n",
      "|        102190|1956-08-13|06:00:00|        9.8|      Y|\n",
      "|        102190|1956-04-19|12:00:00|        9.8|      Y|\n",
      "|        102170|2014-10-02|06:00:00|        9.8|      G|\n",
      "|        102190|1956-08-27|06:00:00|        9.8|      Y|\n",
      "+--------------+----------+--------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Station_number: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Temperature: string (nullable = true)\n",
      " |-- Quality: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_csv = spark.read.csv(\"temperature-readings-tiny.csv\", header=False,sep=\";\")\n",
    "df=temp_csv.selectExpr(\"_c0 as Station_number\", \"_c1 as Date\", \"_c2 as Time\", \"_c3 as Temperature\",\"_c4 as Quality\")\n",
    "df=df.withColumn('Date',F.to_date(\"Date\", \"yyyy-MM-dd\"))\n",
    "df=df.orderBy(df.Temperature,ascending=False)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|year|max_Temperature|\n",
      "+----+---------------+\n",
      "|2014|           29.1|\n",
      "|1960|           29.0|\n",
      "|1959|           28.2|\n",
      "|1958|           28.1|\n",
      "|1956|           26.0|\n",
      "|1957|           25.2|\n",
      "|1955|           20.4|\n",
      "|1961|           19.0|\n",
      "|2013|           10.2|\n",
      "+----+---------------+\n",
      "\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- max_Temperature: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# maximun temperature per year\n",
    "df.createOrReplaceTempView(\"temps_table\")\n",
    "query_max_temps=\"SELECT YEAR(Date) as year, MAX(FLOAT(Temperature)) AS max_Temperature \\\n",
    "FROM temps_table WHERE YEAR(Date) >=1950 AND YEAR(Date) <= 2014 GROUP BY year ORDER BY max_Temperature DESC\"\n",
    "\n",
    "max_temps=spark.sql(query_max_temps)\n",
    "max_temps.show()\n",
    "max_temps.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|year|min_Temperature|\n",
      "+----+---------------+\n",
      "|1956|          -30.0|\n",
      "|1960|          -28.3|\n",
      "|1958|          -27.9|\n",
      "|1955|          -26.2|\n",
      "|2014|          -24.3|\n",
      "|1961|          -23.5|\n",
      "|1959|          -23.2|\n",
      "|1957|          -19.9|\n",
      "|2013|          -13.3|\n",
      "+----+---------------+\n",
      "\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- min_Temperature: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# min temperatures per year\n",
    "query_min_temps=\"SELECT YEAR(Date) as year, MIN(FLOAT(Temperature)) AS min_Temperature \\\n",
    "FROM temps_table WHERE YEAR(Date) >=1950 AND YEAR(Date) <= 2014 GROUP BY year ORDER BY min_Temperature ASC\"\n",
    "\n",
    "min_temps=spark.sql(query_min_temps)\n",
    "min_temps.show()\n",
    "min_temps.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---------------+\n",
      "|year|Station_number|max_Temperature|\n",
      "+----+--------------+---------------+\n",
      "|2014|        102170|           29.1|\n",
      "|1960|        102190|           29.0|\n",
      "|1959|        102190|           28.2|\n",
      "|1958|        102190|           28.1|\n",
      "|1956|        102190|           26.0|\n",
      "|1957|        102190|           25.2|\n",
      "|1955|        102190|           20.4|\n",
      "|1961|        102190|           19.0|\n",
      "|2013|        102170|           10.2|\n",
      "+----+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max temperature for every station per year\n",
    "query_max_temps_station=\"SELECT YEAR(Date) as year, Station_number,MAX(FLOAT(Temperature)) AS max_Temperature \\\n",
    "FROM temps_table WHERE YEAR(Date) >=1950 AND YEAR(Date) <= 2014 GROUP BY year ,Station_number ORDER BY max_Temperature DESC\"\n",
    "\n",
    "max_temps_station=spark.sql(query_max_temps_station)\n",
    "max_temps_station.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---------------+\n",
      "|year|Station_number|min_Temperature|\n",
      "+----+--------------+---------------+\n",
      "|1956|        102190|          -30.0|\n",
      "|1960|        102190|          -28.3|\n",
      "|1958|        102190|          -27.9|\n",
      "|1955|        102190|          -26.2|\n",
      "|2014|        102170|          -24.3|\n",
      "|1961|        102190|          -23.5|\n",
      "|1959|        102190|          -23.2|\n",
      "|1957|        102190|          -19.9|\n",
      "|2013|        102170|          -13.3|\n",
      "+----+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# min temperature for every station per year\n",
    "query_min_temps_station=\"SELECT YEAR(Date) as year, Station_number,MIN(FLOAT(Temperature)) AS min_Temperature \\\n",
    "FROM temps_table WHERE YEAR(Date) >=1950 AND YEAR(Date) <= 2014 GROUP BY year ,Station_number ORDER BY min_Temperature ASC\"\n",
    "\n",
    "min_temps_station=spark.sql(query_min_temps_station)\n",
    "min_temps_station.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see Lab 2 ===> solution with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)Count the number of readings for each month in the period of 1950-2014 which are higher than 10 degrees.Repeat the exercise,this time taking only distinct readings from each station. That is, if a station reported are a ding above 10 degrees in some month,then it appears only once in the count for that month.In this exercise you will use the temperature-readings.csvfile.¶\n",
    "#### The out put should contain the following information:Year, month, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|year|month|count|\n",
      "+----+-----+-----+\n",
      "|1955|    9|   55|\n",
      "|1955|   10|   23|\n",
      "|1956|    3|    4|\n",
      "|1956|    4|    7|\n",
      "|1956|    5|   60|\n",
      "|1956|    6|   92|\n",
      "|1956|    7|  108|\n",
      "|1956|    8|   84|\n",
      "|1956|    9|   54|\n",
      "|1956|   10|   17|\n",
      "|1957|    3|    1|\n",
      "|1957|    4|   13|\n",
      "|1957|    5|   46|\n",
      "|1957|    6|   72|\n",
      "|1957|    7|  109|\n",
      "|1957|    8|   96|\n",
      "|1957|    9|   43|\n",
      "|1957|   10|    9|\n",
      "|1958|    4|    4|\n",
      "|1958|    5|   40|\n",
      "+----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using the temps_table from before\n",
    "\n",
    "query_over10=\" SELECT YEAR(Date) AS year , MONTH(Date) AS month ,COUNT(Temperature) AS count \\\n",
    "FROM temps_table WHERE FLOAT(Temperature) >=10 AND YEAR(Date) >=1950 AND YEAR(Date) <= 2014  GROUP BY year,month ORDER BY year,month ASC\"\n",
    "\n",
    "spark.sql(query_over10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "|year|month|             count|\n",
      "+----+-----+------------------+\n",
      "|1955|    9| 14.57714285714286|\n",
      "|1955|   10|12.440000000000001|\n",
      "|1956|    3|             12.05|\n",
      "|1956|    4|             11.72|\n",
      "|1956|    5| 14.83181818181818|\n",
      "|1956|    6|15.277586206896546|\n",
      "|1956|    7|17.165789473684207|\n",
      "|1956|    8|13.993877551020407|\n",
      "|1956|    9|              13.1|\n",
      "|1956|   10|11.753846153846153|\n",
      "|1957|    3|              10.6|\n",
      "|1957|    4|12.246153846153847|\n",
      "|1957|    5| 14.45151515151515|\n",
      "|1957|    6|16.168627450980395|\n",
      "|1957|    7|17.081159420289858|\n",
      "|1957|    8|              15.9|\n",
      "|1957|    9|12.411111111111113|\n",
      "|1957|   10|            11.475|\n",
      "|1958|    4|              12.1|\n",
      "|1958|    5|13.406451612903224|\n",
      "+----+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now using distinct temperatures\n",
    "\n",
    "query_over10_distinct=\" SELECT YEAR(Date) AS year , MONTH(Date) AS month ,AVG(DISTINCT(Temperature)) AS count \\\n",
    "FROM temps_table WHERE FLOAT(Temperature) >=10 AND YEAR(Date) >=1950 AND YEAR(Date) <= 2014 GROUP BY year,month ORDER BY year,month ASC\"\n",
    "\n",
    "spark.sql(query_over10_distinct).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3)Find the average monthly temperature for each available station in Sweden. Your result should include average temperature for each station for each month in the period of 1960-2014. Bear in mind that not every station has the readings for each month in this timeframe.In this exercise you will use the temperature-readings.csvfile.¶\n",
    "#### The output should contain the following information: Year, month, station number, average monthly temperature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The monthly average is calculated by averaging the daily maximums and minimums for that month.For example, to get the monthly average for October, take maximums and minimums for each day, sum them up and divide by 62 (which is the same as taking the daily averages, summing them up and divide by the number of days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+--------------------+\n",
      "|year|month|station|         avg_monthly|\n",
      "+----+-----+-------+--------------------+\n",
      "|2014|   12| 102170|  -5.801612894020734|\n",
      "|2014|   11| 102170|  2.5249999977648256|\n",
      "|2014|   10| 102170|   7.106451625304837|\n",
      "|2014|    9| 102170|   8.584999987483025|\n",
      "|2014|    8| 102170|   13.86935483255694|\n",
      "|2014|    7| 102170|    19.6596774132021|\n",
      "|2014|    6| 102170|  14.443333387374878|\n",
      "|2014|    5| 102170|  10.756451619728919|\n",
      "|2014|    4| 102170|   4.776666717727979|\n",
      "|2014|    3| 102170|  1.8967741969371996|\n",
      "|2014|    2| 102170|  0.6749999944918922|\n",
      "|2014|    1| 102170|  -4.106451601751389|\n",
      "|2013|   12| 102170|  0.7096774189943268|\n",
      "|2013|   11| 102170|-0.05166668320695559|\n",
      "|1961|    5| 102190|  10.615333366394044|\n",
      "|1961|    4| 102190|   4.507777822679944|\n",
      "|1961|    3| 102190|  2.7086021617375398|\n",
      "|1961|    2| 102190|  -2.133333349955224|\n",
      "|1961|    1| 102190|  -7.215053754468119|\n",
      "|1960|   12| 102190| -2.4274193498115704|\n",
      "+----+-----+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=\" WITH sub AS (SELECT YEAR(Date) AS year , MONTH(Date) AS month , DAY(Date) AS day,Station_number AS station,AVG(FLOAT(Temperature)) AS avg_daily \\\n",
    "FROM temps_table WHERE YEAR(Date) >=1950 AND YEAR(Date) <= 2014 GROUP BY year,month,day,station ORDER BY (year,month,day) DESC) \\\n",
    "SELECT year,month,station,AVG(avg_daily) AS avg_monthly FROM sub GROUP BY year,month,station \\\n",
    "ORDER BY (year,month,station) DESC\"\n",
    "\n",
    "spark.sql(query).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Provide a list of stations with their associated maximum measured temperatures and maximum measured daily precipitation. Show only those stations where the maximum temperature is between 25 and 30 degrees and maximum daily precipitation is between 100 mm and 200mm.In this exercise you will use the temperature-readings.csvand precipitation-readings.csvfiles.The output should contain the following information: Station number, maximum measured temperature, maximum daily precipitation¶\n",
    "#### HINT: The correct result for this question should be empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------+-------------+-------+\n",
      "|Station_number|      Date|    Time|Precipitation|Quality|\n",
      "+--------------+----------+--------+-------------+-------+\n",
      "|         94390|1998-08-13|11:00:00|          9.9|      Y|\n",
      "|         93520|2008-08-03|14:00:00|          9.9|      Y|\n",
      "|         97280|2006-08-01|04:00:00|          9.9|      Y|\n",
      "|         93520|1996-05-15|00:00:00|          9.9|      Y|\n",
      "|         96040|2002-06-24|12:00:00|          9.9|      Y|\n",
      "|        173900|2003-05-14|19:00:00|          9.9|      Y|\n",
      "|         97280|2001-08-08|23:00:00|          9.9|      Y|\n",
      "|        167990|2014-07-16|18:00:00|          9.9|      G|\n",
      "|        172940|2016-06-13|17:00:00|          9.9|      G|\n",
      "|         93520|2008-08-03|13:00:00|          9.9|      Y|\n",
      "|         99280|2013-09-17|20:00:00|          9.9|      G|\n",
      "|        117430|2014-08-18|01:00:00|          9.9|      G|\n",
      "|        103100|2002-06-11|15:00:00|          9.9|      Y|\n",
      "|        106160|2011-06-23|12:00:00|          9.9|      Y|\n",
      "|        166910|2011-07-19|04:00:00|          9.9|      Y|\n",
      "|        114140|2011-08-15|22:00:00|          9.9|      Y|\n",
      "|        159880|2000-06-07|18:00:00|          9.9|      Y|\n",
      "|        104580|2012-07-01|15:00:00|          9.9|      Y|\n",
      "|        163900|2005-07-28|02:00:00|          9.9|      Y|\n",
      "|        116490|2005-08-10|07:00:00|          9.9|      Y|\n",
      "+--------------+----------+--------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Station_number: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Precipitation: string (nullable = true)\n",
      " |-- Quality: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using temps_table from above\n",
    "# make a table for the presipitation\n",
    "precip_csv=spark.read.csv(\"C:/Users/quartermaine/Documents/Big Data Analytics Labs/precipitation-readings.csv\", header=False,sep=\";\")\n",
    "dt=precip_csv.selectExpr(\"_c0 as Station_number\", \"_c1 as Date\", \"_c2 as Time\", \"_c3 as Precipitation\",\"_c4 as Quality\")\n",
    "dt=dt.withColumn('Date',F.to_date(\"Date\", \"yyyy-MM-dd\"))\n",
    "dt=dt.orderBy(dt.Precipitation,ascending=False)\n",
    "dt.show()\n",
    "dt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.createOrReplaceTempView(\"precips_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------+\n",
      "|Station_number|Max_Precip|Max_Temp|\n",
      "+--------------+----------+--------+\n",
      "+--------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_temp_precip=\"WITH sub AS (SELECT t.Station_number,t.Date,t.Temperature,p.Precipitation \\\n",
    "FROM temps_table AS t , precips_table AS p WHERE p.Station_number=t.Station_number) \\\n",
    "SELECT Station_number , MAX(FLOAT(Precipitation)) AS Max_Precip,MAX(FLOAT(Temperature)) AS Max_Temp \\\n",
    "FROM sub  GROUP BY Station_number,YEAR(Date),MONTH(date),DAY(Date) HAVING MAX(FLOAT(Temperature)) BETWEEN 20 AND 30 \\\n",
    "AND MAX(FLOAT(Precipitation)) BETWEEN 10 AND 20\"\n",
    "\n",
    "spark.sql(query_temp_precip).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5)Calculate the average monthly precipitation for the Östergotland region (list of stations is provided in the separate file)for the period1993-2016. In order to do this, you will first need to calculate the total monthly precipitation for each station before calculating the monthly average (by averaging over stations).In this exercise you will use the precipitation-readings.csvand stations-Ostergotland.csv files. HINT(not for the SparkSQL lab): Avoid using joins here! stations-Ostergotland.csv is small and if distributed will cause a number of unnecessary shuffles when joined with precipitationRDD. If you distribute precipitation-readings.csv then either repartition your stations RDD to 1 partition or make use of the collect to acquire a python list and broadcast function to broadcast the list to all nodes.¶\n",
    "#### The output should contain the following information: Year,month,average monthly precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------------+--------+----------+-------------------+-------------------+---------+\n",
      "|Station_number|        Station_name|Measurment_height|Latitude|Longtitude|      Readings_from|         Redings_to|Elevation|\n",
      "+--------------+--------------------+-----------------+--------+----------+-------------------+-------------------+---------+\n",
      "|         84260|             Örberga|              2.0| 58.4274|    14.826|1990-11-01 00:00:00|1993-03-31 23:59:59|    105.0|\n",
      "|         85630|           Västra Ny|              2.0|   58.65|     15.05|1961-01-01 00:00:00|1966-12-31 23:59:59|    120.0|\n",
      "|         85460|         Kettstaka A|              2.0| 58.7165|     15.03|1995-08-01 00:00:00|2016-10-01 08:00:00|    225.0|\n",
      "|         85450|            Godegård|              2.0| 58.7936|   15.1694|1961-01-01 00:00:00|1988-02-29 23:59:59|    130.0|\n",
      "|         85490|          Zinkgruvan|              2.0| 58.8063|   15.1285|1983-09-01 00:00:00|2016-08-31 23:59:59|    215.0|\n",
      "|         85650|          Kärnskogen|              2.0|   58.85|   15.2833|1951-01-01 00:00:00|1963-04-30 23:59:59|    108.0|\n",
      "|         85390|            Kvarn Mo|              2.0| 58.6277|   15.4372|2010-03-01 00:00:00|2016-10-01 08:00:00|     72.0|\n",
      "|         85410|            Finspång|              2.0| 58.7104|   15.8215|1990-11-01 00:00:00|1993-03-31 23:59:59|     41.0|\n",
      "|         86470|          Simonstorp|              2.0|58.78468|  16.15439|2014-06-01 00:00:00|2014-12-03 23:59:59|     80.0|\n",
      "|         86440|         Stavsjö Aut|              2.0| 58.7333|   16.3667|1993-07-01 00:00:00|1996-03-01 23:59:59|     60.0|\n",
      "|         86420|Kolmården-Strömsf...|              2.0| 58.6894|   16.3103|1995-12-15 00:00:00|2016-10-01 08:00:00|    153.0|\n",
      "|         86340|     Norrköping-SMHI|              2.0| 58.5833|     16.15|1993-04-28 00:00:00|2013-08-04 23:59:59|     21.0|\n",
      "|         86350|Norrköping-Kungsä...|              2.0| 58.5842|   16.2383|1972-10-01 00:00:00|1980-08-31 23:59:59|      5.0|\n",
      "|         86360|          Norrköping|              2.0|  58.606|   16.2127|1961-01-01 00:00:00|1964-03-31 23:59:59|      3.0|\n",
      "|         86370|    Norrköping-Sörby|              2.0| 58.6082|   16.1212|1949-01-01 00:00:00|1993-06-30 23:59:59|     27.0|\n",
      "|         86200|               Holma|              2.0| 58.3339|   16.8157|1962-05-01 00:00:00|1996-11-30 23:59:59|      5.0|\n",
      "|         86330|            Marviken|              2.0| 58.5491|   16.8157|1965-05-01 00:00:00|2002-06-30 23:59:59|     10.0|\n",
      "|         87140|          Harstena A|              2.0| 58.2505|   17.0106|1995-12-15 00:00:00|2016-10-01 08:00:00|     14.0|\n",
      "|         87150|            Harstena|              2.0| 58.2502|   17.0095|1951-01-01 00:00:00|2001-12-10 23:59:59|      5.0|\n",
      "|         86130|        Valdemarsvik|              2.0| 58.2148|   16.5828|1990-11-01 00:00:00|1993-03-31 23:59:59|     10.0|\n",
      "+--------------+--------------------+-----------------+--------+----------+-------------------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Station_number: string (nullable = true)\n",
      " |-- Station_name: string (nullable = true)\n",
      " |-- Measurment_height: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longtitude: string (nullable = true)\n",
      " |-- Readings_from: string (nullable = true)\n",
      " |-- Redings_to: string (nullable = true)\n",
      " |-- Elevation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using the precips_table from above \n",
    "# make table for Ostegorland\n",
    "ost_csv=spark.read.csv(\"C:/Users/quartermaine/Documents/Big Data Analytics Labs/stations-Ostergotland.csv\", header=False,sep=\";\")\n",
    "DF=ost_csv.selectExpr(\"_c0 as Station_number\", \"_c1 as Station_name \",\"_c2 as Measurment_height\", \n",
    "                         \"_c3 as Latitude\",\"_c4 as Longtitude\",\"_c5 as Readings_from\",\"_c6 as Redings_to\",\"_c7 as Elevation\")\n",
    "\n",
    "DF.show()\n",
    "DF.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.createOrReplaceTempView(\"ost_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+\n",
      "|year|month|   avg_precipitation|\n",
      "+----+-----+--------------------+\n",
      "|1993|    4|                 0.0|\n",
      "|1993|    5|0.028513513565868946|\n",
      "|1993|    6| 0.09323432421920323|\n",
      "|1993|    7| 0.12874494014400864|\n",
      "|1993|    8| 0.11414427218545108|\n",
      "|1993|    9| 0.06023738923474306|\n",
      "|1993|   10| 0.05814266531736982|\n",
      "|1993|   11| 0.06053748258944111|\n",
      "|1993|   12| 0.05167130978575656|\n",
      "|1994|    1| 0.04062500083819032|\n",
      "|1994|    2|0.033936651943316104|\n",
      "|1994|    3|  0.0516483520802397|\n",
      "|1994|    4|0.032719547039810726|\n",
      "|1994|    5|  0.0349582173967926|\n",
      "|1994|    6| 0.06442857197352818|\n",
      "|1994|    7|                 0.0|\n",
      "|1994|    8| 0.08155339828145984|\n",
      "|1994|    9| 0.13611510657578063|\n",
      "|1994|   10| 0.04486486480848209|\n",
      "|1994|   11|0.021835883330652214|\n",
      "+----+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I didn't use the approach we use in the 3rd question but the classical definition!!!!!\n",
    "\n",
    "query_max_precip=\"WITH sub AS(SELECT Station_number,YEAR(Date) AS year,MONTH(Date) AS month ,DAY(Date) AS day,Precipitation \\\n",
    "FROM precips_table  WHERE Station_number IN (SELECT Station_number FROM ost_table) ) \\\n",
    "SELECT year, month,AVG(FLOAT(Precipitation)) AS avg_precipitation FROM sub \\\n",
    "WHERE INT(year) BETWEEN 1993 AND 2016 GROUP BY year,month ORDER BY year ASC,month ASC\"\n",
    "\n",
    "spark.sql(query_max_precip).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the avg precipication like the temperature in question 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------------------+\n",
      "|year|month|day|           avg_daily|\n",
      "+----+-----+---+--------------------+\n",
      "|2007|    9| 22| 0.17222222064932188|\n",
      "|2011|    8| 10| 0.08333333405769533|\n",
      "|1999|    9| 20|0.020567376562889587|\n",
      "|2006|    8| 23| 0.08661971902343589|\n",
      "|2002|    6| 30| 0.07014925422063514|\n",
      "|2005|   12|  1|  0.0729166663562258|\n",
      "|1999|   12| 31| 0.10559440633425346|\n",
      "|2007|    1| 18|  0.0708333344405724|\n",
      "|2002|   12| 28| 0.18333333606521288|\n",
      "|2015|   10| 22|0.021142857117312296|\n",
      "|1996|   11| 28| 0.19014084900558834|\n",
      "|1997|    5| 13|0.022916666852931183|\n",
      "|1999|    6| 25| 0.04236111158712043|\n",
      "|2014|    2| 10|  0.0467741939649787|\n",
      "|1996|    6|  1| 0.03623188454387845|\n",
      "|1993|    8|  9| 0.07916666784634192|\n",
      "|2006|    2|  3|0.013888888940629032|\n",
      "|1996|    1| 19|0.013043478563211966|\n",
      "|2014|   12| 17| 0.02513661247784974|\n",
      "|2005|   12| 25|0.017293233396415423|\n",
      "+----+-----+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- avg_daily: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q2=\"WITH sub AS( SELECT YEAR(Date) AS year,MONTH(Date) AS month ,DAY(Date) AS day,Station_number,Precipitation \\\n",
    "FROM precips_table WHERE Station_number IN (SELECT Station_number FROM ost_table) ) \\\n",
    "SELECT year,month,day,AVG(FLOAT(Precipitation)) AS avg_daily FROM sub \\\n",
    "WHERE INT(year) BETWEEN 1993 AND 2016 GROUP BY year,month,day\"\n",
    "\n",
    "T2=spark.sql(q2)\n",
    "T2.show()\n",
    "T2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2.createOrReplaceTempView(\"T2_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+\n",
      "|year|month|  avg_monthly_precip|\n",
      "+----+-----+--------------------+\n",
      "|1993|    4|                 0.0|\n",
      "|1993|    5|0.028360215105837392|\n",
      "|1993|    6| 0.08851506861297961|\n",
      "|1993|    7|  0.1282258073208473|\n",
      "|1993|    8| 0.13591977530813945|\n",
      "|1993|    9| 0.05759920684679869|\n",
      "|1993|   10|0.058152174357674905|\n",
      "|1993|   11| 0.05952020228624042|\n",
      "|1993|   12| 0.04989481123243538|\n",
      "|1994|    1| 0.04071698705971345|\n",
      "|1994|    2|0.033709180670613854|\n",
      "|1994|    3| 0.05174230323386763|\n",
      "|1994|    4| 0.03295634947273703|\n",
      "|1994|    5| 0.03373655926195845|\n",
      "|1994|    6| 0.06393115995038705|\n",
      "|1994|    7|                 0.0|\n",
      "|1994|    8| 0.08052155520122158|\n",
      "|1994|    9| 0.13360988390269843|\n",
      "|1994|   10| 0.04476390831005273|\n",
      "|1994|   11|0.021805555714915194|\n",
      "+----+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q=\"SELECT year,month,AVG(avg_daily) AS avg_monthly_precip FROM T2_table GROUP BY year,month ORDER BY year ASC,month ASC\"\n",
    "spark.sql(Q).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
