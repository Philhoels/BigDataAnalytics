{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDA1 - Spark - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Student: Phillip Hölscher__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this set of exercises you will work exclusively with Spark. This means that in your programs, you only need to create the SparkContext .\n",
    "In a number of exercises you will be asked to calculated temperature averages (daily and monthly). These are not always computed according to the standard definition of ‘average’. In this domain the daily average temperature is calculated by averaging the daily measured maximum and the daily measured minimum temperatures. The monthly average is calculated by averaging the daily maximums and minimums for that month. For example, to get the monthly average for October, take maximums and minimums for each day, sum them up and divide by 62 (which is the same as taking the daily averages, summing them up and divide by the number of days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make set up\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Working path\"\n",
    "#path /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/temperature-readings.csv\" 64728769L, 2146781232C\u001b[?2004l\n",
      "\"~/Desktop/temperature-readings.csv\" 64728769L, 2146781232C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/precipitation-readings.csv\" 20292769L, 661060228C\u001b[?2004l\n",
      "\"~/Desktop/precipitation-readings.csv\" 20292769L, 661060228C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/stations-Ostergotland.csv\" [dos] 34L, 2860C\u001b[?2004l\n",
      "\"~/Desktop/stations-Ostergotland.csv\" [dos] 34L, 2860C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/stations.csv\" 812L, 67697C\u001b[?2004l\n",
      "\"~/Desktop/stations.csv\" 812L, 67697C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8"
     ]
    }
   ],
   "source": [
    "# Remove UTF-8 BOM\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/temperature-readings.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/precipitation-readings.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/stations-Ostergotland.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/stations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIM - Vi IMproved 8.0 (2016 Sep 12, compiled Aug 17 2018 17:24:51)\n",
      "Too many edit arguments: \"-\"\n",
      "More info with: \"vim -h\"\n",
      "VIM - Vi IMproved 8.0 (2016 Sep 12, compiled Aug 17 2018 17:24:51)\n",
      "Too many edit arguments: \"-\"\n",
      "More info with: \"vim -h\"\n",
      "VIM - Vi IMproved 8.0 (2016 Sep 12, compiled Aug 17 2018 17:24:51)\n",
      "Too many edit arguments: \"-\"\n",
      "More info with: \"vim -h\"\n",
      "VIM - Vi IMproved 8.0 (2016 Sep 12, compiled Aug 17 2018 17:24:51)\n",
      "Too many edit arguments: \"-\"\n",
      "More info with: \"vim -h\"\n"
     ]
    }
   ],
   "source": [
    "# Remove UTF-8 BOM\n",
    "'''\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/temperature-readings.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/precipitation-readings.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/stations-Ostergotland.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2/stations.csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/temperature-readings-tiny.csv\" 10000L, 336854C\u001b[?2004l\n",
      "\"~/Desktop/temperature-readings-tiny.csv\" 10000L, 336854C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"~/Desktop/precipitation-readings-tiny.csv\" 10000L, 330001C\u001b[?2004l\n",
      "\"~/Desktop/precipitation-readings-tiny.csv\" 10000L, 330001C written\n",
      "\u001b[?2004l\u001b[?1l\u001b>\u001b[?25h\u001b[2J\u001b[?47l\u001b8"
     ]
    }
   ],
   "source": [
    "# Remove UTF-8 BOM\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/temperature-readings-tiny.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/precipitation-readings-tiny.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove UTF-8 BOM\n",
    "'''\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/temperature-readings-tiny.csv\n",
    "!vim -c ':set nobomb' -c ':set fileencoding=utf-8' -c ':wq' /Users/phillipholscher/Desktop/precipitation-readings-tiny.csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/phillipholscher/Desktop/temperature-readings-tiny.csv\"\n",
    "path_unifolder = \"/Users/phillipholscher/OneDrive - Linköpings universitet/LiU/Statistics and Machine Learning/Spring19T2/Big Data Analytics/Labs/Lab 2temperature-readings-tiny.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "- What are the lowest and highest temperatures measured each year for the period 1950-2014. Provide the lists sorted in the descending order with respect to the maximum temperature. In this exercise you will use the temperature-readings.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext \n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.AssertionError: assertion failed: Expected hostname (not IP) but got fe80:0:0:0:619:1c79:9e10:ad28%10\n\tat scala.Predef$.assert(Predef.scala:170)\n\tat org.apache.spark.util.Utils$.checkHost(Utils.scala:982)\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:31)\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:155)\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:59)\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:126)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:164)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:500)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-783ce20b5f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"Lab_BDA1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 118\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \"\"\"\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1525\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.AssertionError: assertion failed: Expected hostname (not IP) but got fe80:0:0:0:619:1c79:9e10:ad28%10\n\tat scala.Predef$.assert(Predef.scala:170)\n\tat org.apache.spark.util.Utils$.checkHost(Utils.scala:982)\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:31)\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:155)\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:59)\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:126)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:164)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:500)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(appName= \"Lab_BDA1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8bea6f735109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemp\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# read the file\n",
    "temp  = sc.textFile(path)\n",
    "lines = temp.map(lambda line: line.split(\";\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with year and temperature\n",
    "year_temperature = lines.map(lambda x: (x[1][0:4], float(x[3])))\n",
    "year_temperature_timeinterval = year_temperature.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max temperature \n",
    "max_temperature = year_temperature_timeinterval.reduceByKey(lambda a,b: a if a>=b else b)\n",
    "max_temperature_sort = max_temperature.sortBy(ascending = False, keyfunc = lambda k: k[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the min temperature\n",
    "min_temperature = year_temperature_timeinterval.reduceByKey(lambda a,b: a if a<=b else b)\n",
    "min_temperature_sort = min_temperature.sortBy(ascending = True, keyfunc = lambda k: k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This are the first 10 of the max temperature: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2014', 29.1),\n",
       " ('1960', 29.0),\n",
       " ('1959', 28.2),\n",
       " ('1958', 28.1),\n",
       " ('1956', 26.0),\n",
       " ('1957', 25.2),\n",
       " ('1955', 20.4),\n",
       " ('1961', 19.0),\n",
       " ('2013', 10.2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 10 - max\n",
    "n = 10\n",
    "print(\"This are the first {} of the max temperature: \".format(n))\n",
    "max_temperature_sort.take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This are the first 10 of the min temperature: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1956', -30.0),\n",
       " ('1960', -28.3),\n",
       " ('1958', -27.9),\n",
       " ('1955', -26.2),\n",
       " ('2014', -24.3),\n",
       " ('1961', -23.5),\n",
       " ('1959', -23.2),\n",
       " ('1957', -19.9),\n",
       " ('2013', -13.3)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 10 - min\n",
    "print(\"This are the first {} of the min temperature: \".format(n))\n",
    "min_temperature_sort.take(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a) Extend the program to include the station number (not the station name) where the maximum/minimum temperature was measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with station & year and temperature\n",
    "year_temperature_station = lines.map(lambda x: (x[1][0:4], (float(x[3]), x[0])))\n",
    "\n",
    "# later on we use reduce by key - we have to create a key (year) by using a nested tuple\n",
    "year_temperature_timeinterval_station = year_temperature_station.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2013', (6.8, '102170')),\n",
       " ('2013', (3.8, '102170')),\n",
       " ('2013', (5.8, '102170')),\n",
       " ('2013', (-1.1, '102170')),\n",
       " ('2013', (-0.2, '102170'))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first 10 year, temperature & station \n",
    "year_temperature_timeinterval_station.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max temperature including the station number\n",
    "max_temperature_station = year_temperature_timeinterval_station.reduceByKey(lambda a,b: a if a>=b else b)\n",
    "max_temperature_station_sort = max_temperature_station.sortBy(ascending = False, keyfunc = lambda k: k[1][0])\n",
    "\n",
    "# get the min temperature including the station number\n",
    "min_temperature_station = year_temperature_timeinterval_station.reduceByKey(lambda a,b: a if a<=b else b)\n",
    "min_temperature_station_sort = min_temperature_station.sortBy(ascending = True, keyfunc = lambda k: k[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This are the first 10 of the max temperature including the station number: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2014', (29.1, '102170')),\n",
       " ('1960', (29.0, '102190')),\n",
       " ('1959', (28.2, '102190')),\n",
       " ('1958', (28.1, '102190')),\n",
       " ('1956', (26.0, '102190')),\n",
       " ('1957', (25.2, '102190')),\n",
       " ('1955', (20.4, '102190')),\n",
       " ('1961', (19.0, '102190')),\n",
       " ('2013', (10.2, '102170'))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 10 - max\n",
    "n = 10\n",
    "print(\"This are the first {} of the max temperature including the station number: \".format(n))\n",
    "max_temperature_station_sort.take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This are the first 10 of the min temperature including the station number: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1956', (-30.0, '102190')),\n",
       " ('1960', (-28.3, '102190')),\n",
       " ('1958', (-27.9, '102190')),\n",
       " ('1955', (-26.2, '102190')),\n",
       " ('2014', (-24.3, '102170')),\n",
       " ('1961', (-23.5, '102190')),\n",
       " ('1959', (-23.2, '102190')),\n",
       " ('1957', (-19.9, '102190')),\n",
       " ('2013', (-13.3, '102170'))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 10 - min\n",
    "print(\"This are the first {} of the min temperature including the station number: \".format(n))\n",
    "min_temperature_station_sort.take(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b) (not for the SparkSQL lab) Write the non-parallelized program in Python to find the maximum temperatures for each year without using Spark. In this case you will run the program using:\n",
    "python script.py\n",
    "This program will read the local file (not from HDFS). The local file is available under /nfshome/hadoop_examples/shared_data/temperatures-big.csv.\n",
    "How does the runtime compare to the Spark version? Use logging (add the --conf spark.eventLog.enabled=true flag) to check the execution of the Spark program. Repeat the exercise, this time using temperatures-big.csv file available on hdfs. Explain the differences and try to reason why such runtimes were observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Temp\n",
      "(2014, (29.1, '102170'))\n",
      "(1960, (29.0, '102190'))\n",
      "(1959, (28.2, '102190'))\n",
      "(1958, (28.1, '102190'))\n",
      "(1956, (26.0, '102190'))\n",
      "(1957, (25.2, '102190'))\n",
      "(1955, (20.4, '102190'))\n",
      "(1961, (19.0, '102190'))\n",
      "(2013, (10.2, '102170'))\n",
      "(1950, (-100, ''))\n",
      "(1951, (-100, ''))\n",
      "(1952, (-100, ''))\n",
      "(1953, (-100, ''))\n",
      "(1954, (-100, ''))\n",
      "(1962, (-100, ''))\n",
      "(1963, (-100, ''))\n",
      "(1964, (-100, ''))\n",
      "(1965, (-100, ''))\n",
      "(1966, (-100, ''))\n",
      "(1967, (-100, ''))\n",
      "(1968, (-100, ''))\n",
      "(1969, (-100, ''))\n",
      "(1970, (-100, ''))\n",
      "(1971, (-100, ''))\n",
      "(1972, (-100, ''))\n",
      "(1973, (-100, ''))\n",
      "(1974, (-100, ''))\n",
      "(1975, (-100, ''))\n",
      "(1976, (-100, ''))\n",
      "(1977, (-100, ''))\n",
      "(1978, (-100, ''))\n",
      "(1979, (-100, ''))\n",
      "(1980, (-100, ''))\n",
      "(1981, (-100, ''))\n",
      "(1982, (-100, ''))\n",
      "(1983, (-100, ''))\n",
      "(1984, (-100, ''))\n",
      "(1985, (-100, ''))\n",
      "(1986, (-100, ''))\n",
      "(1987, (-100, ''))\n",
      "(1988, (-100, ''))\n",
      "(1989, (-100, ''))\n",
      "(1990, (-100, ''))\n",
      "(1991, (-100, ''))\n",
      "(1992, (-100, ''))\n",
      "(1993, (-100, ''))\n",
      "(1994, (-100, ''))\n",
      "(1995, (-100, ''))\n",
      "(1996, (-100, ''))\n",
      "(1997, (-100, ''))\n",
      "(1998, (-100, ''))\n",
      "(1999, (-100, ''))\n",
      "(2000, (-100, ''))\n",
      "(2001, (-100, ''))\n",
      "(2002, (-100, ''))\n",
      "(2003, (-100, ''))\n",
      "(2004, (-100, ''))\n",
      "(2005, (-100, ''))\n",
      "(2006, (-100, ''))\n",
      "(2007, (-100, ''))\n",
      "(2008, (-100, ''))\n",
      "(2009, (-100, ''))\n",
      "(2010, (-100, ''))\n",
      "(2011, (-100, ''))\n",
      "(2012, (-100, ''))\n",
      "Min Temp\n",
      "(1950, (100, ''))\n",
      "(1951, (100, ''))\n",
      "(1952, (100, ''))\n",
      "(1953, (100, ''))\n",
      "(1954, (100, ''))\n",
      "(1962, (100, ''))\n",
      "(1963, (100, ''))\n",
      "(1964, (100, ''))\n",
      "(1965, (100, ''))\n",
      "(1966, (100, ''))\n",
      "(1967, (100, ''))\n",
      "(1968, (100, ''))\n",
      "(1969, (100, ''))\n",
      "(1970, (100, ''))\n",
      "(1971, (100, ''))\n",
      "(1972, (100, ''))\n",
      "(1973, (100, ''))\n",
      "(1974, (100, ''))\n",
      "(1975, (100, ''))\n",
      "(1976, (100, ''))\n",
      "(1977, (100, ''))\n",
      "(1978, (100, ''))\n",
      "(1979, (100, ''))\n",
      "(1980, (100, ''))\n",
      "(1981, (100, ''))\n",
      "(1982, (100, ''))\n",
      "(1983, (100, ''))\n",
      "(1984, (100, ''))\n",
      "(1985, (100, ''))\n",
      "(1986, (100, ''))\n",
      "(1987, (100, ''))\n",
      "(1988, (100, ''))\n",
      "(1989, (100, ''))\n",
      "(1990, (100, ''))\n",
      "(1991, (100, ''))\n",
      "(1992, (100, ''))\n",
      "(1993, (100, ''))\n",
      "(1994, (100, ''))\n",
      "(1995, (100, ''))\n",
      "(1996, (100, ''))\n",
      "(1997, (100, ''))\n",
      "(1998, (100, ''))\n",
      "(1999, (100, ''))\n",
      "(2000, (100, ''))\n",
      "(2001, (100, ''))\n",
      "(2002, (100, ''))\n",
      "(2003, (100, ''))\n",
      "(2004, (100, ''))\n",
      "(2005, (100, ''))\n",
      "(2006, (100, ''))\n",
      "(2007, (100, ''))\n",
      "(2008, (100, ''))\n",
      "(2009, (100, ''))\n",
      "(2010, (100, ''))\n",
      "(2011, (100, ''))\n",
      "(2012, (100, ''))\n",
      "(2013, (-13.3, '102170'))\n",
      "(1957, (-19.9, '102190'))\n",
      "(1959, (-23.2, '102190'))\n",
      "(1961, (-23.5, '102190'))\n",
      "(2014, (-24.3, '102170'))\n",
      "(1955, (-26.2, '102190'))\n",
      "(1958, (-27.9, '102190'))\n",
      "(1960, (-28.3, '102190'))\n",
      "(1956, (-30.0, '102190'))\n",
      "Time:\n",
      "0.08859992027282715\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "maxTemp = {}\n",
    "minTemp = {}\n",
    "\n",
    "# Expand the list.\n",
    "for i in range(1950,2015):\n",
    "    maxTemp[i] = (-100, \"\")\n",
    "    minTemp[i] = (+100, \"\")\n",
    "\n",
    "\n",
    "with open(path) as f:\n",
    "    \n",
    "    for line in f:\n",
    "        values = line.split(\";\")\n",
    "        year = int(values[1][:4])\n",
    "\n",
    "        if year >= 1950 and year <= 2014:\n",
    "            if maxTemp[year][0] < float(values[3]):\n",
    "                maxTemp[year] = (float(values[3]), values[0])\n",
    "            if minTemp[year][0] > float(values[3]):\n",
    "                minTemp[year] = (float(values[3]), values[0])\n",
    "\n",
    "\n",
    "maxTempSorted = sorted(maxTemp.items(), key = lambda tup: tup[1][0], reverse=True)\n",
    "minTempSorted = sorted(minTemp.items(), key = lambda tup: tup[1][0], reverse=True)\n",
    "\n",
    "\n",
    "print(\"Max Temp\")\n",
    "print(\"\\n\".join(map(str, maxTempSorted)))\n",
    "\n",
    "print(\"Min Temp\")\n",
    "print(\"\\n\".join(map(str, minTempSorted)))\n",
    "\n",
    "\n",
    "print(\"Time:\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the same output as before \n",
    "# use pandas to do all operations\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tempdata = pd.read_csv(path, sep=';',header=None)\n",
    "tempdata = tempdata.rename(columns={0:\"Station\", 1:\"Year\", 2:\"Time\", 3:\"Temperature\", 4:\"Quality\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# steps: \n",
    "# 1. reduce the df (years, temp)\n",
    "# 2. sort from max to min & min to max\n",
    "# 3. reduce by the years (just the max vlaue of each year)\n",
    "# 4. repeat with df (years, temp, station number)\n",
    "\n",
    "# step 1\n",
    "tempdata_year_temp = tempdata[[\"Year\", \"Temperature\"]]\n",
    "tempdata_year_temp[\"Year\"] = pd.DatetimeIndex(tempdata['Year']).year # get just the year from the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This are the first 10 of the max temperature: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>2014</td>\n",
       "      <td>29.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8816</th>\n",
       "      <td>1960</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>2014</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>1959</td>\n",
       "      <td>28.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>2014</td>\n",
       "      <td>28.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5934</th>\n",
       "      <td>1958</td>\n",
       "      <td>28.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>2014</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>1960</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5927</th>\n",
       "      <td>1958</td>\n",
       "      <td>27.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5938</th>\n",
       "      <td>1958</td>\n",
       "      <td>27.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Temperature\n",
       "529   2014         29.1\n",
       "8816  1960         29.0\n",
       "535   2014         28.7\n",
       "7579  1959         28.2\n",
       "501   2014         28.2\n",
       "5934  1958         28.1\n",
       "527   2014         28.0\n",
       "8967  1960         28.0\n",
       "5927  1958         27.6\n",
       "5938  1958         27.6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2 \n",
    "# sort from max to min\n",
    "tempdata_year_temp_max = tempdata_year_temp.sort_values(\"Temperature\" , ascending=False)\n",
    "\n",
    "n = 10 # print the first 10 rows\n",
    "print(\"This are the first {} of the max temperature: \".format(n))\n",
    "tempdata_year_temp_max.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tempdata_year_dc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e0bd8d115651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sort from min to max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtempdata_year_temp_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempdata_year_dc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Temperature\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;31m# print the first 10 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This are the first {} of the min temperature: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtempdata_year_temp_min\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tempdata_year_dc' is not defined"
     ]
    }
   ],
   "source": [
    "# sort from min to max\n",
    "tempdata_year_temp_min = tempdata_year_dc.sort_values(\"Temperature\" , ascending=True)\n",
    "n = 10 # print the first 10 rows\n",
    "print(\"This are the first {} of the min temperature: \".format(n))\n",
    "tempdata_year_temp_min.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This are the first 10 of the max temperature: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Station</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>2014</td>\n",
       "      <td>29.1</td>\n",
       "      <td>102170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8816</th>\n",
       "      <td>1960</td>\n",
       "      <td>29.0</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>2014</td>\n",
       "      <td>28.7</td>\n",
       "      <td>102170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>1959</td>\n",
       "      <td>28.2</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>2014</td>\n",
       "      <td>28.2</td>\n",
       "      <td>102170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5934</th>\n",
       "      <td>1958</td>\n",
       "      <td>28.1</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>2014</td>\n",
       "      <td>28.0</td>\n",
       "      <td>102170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>1960</td>\n",
       "      <td>28.0</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5927</th>\n",
       "      <td>1958</td>\n",
       "      <td>27.6</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5938</th>\n",
       "      <td>1958</td>\n",
       "      <td>27.6</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Temperature  Station\n",
       "529   2014         29.1   102170\n",
       "8816  1960         29.0   102190\n",
       "535   2014         28.7   102170\n",
       "7579  1959         28.2   102190\n",
       "501   2014         28.2   102170\n",
       "5934  1958         28.1   102190\n",
       "527   2014         28.0   102170\n",
       "8967  1960         28.0   102190\n",
       "5927  1958         27.6   102190\n",
       "5938  1958         27.6   102190"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 4\n",
    "tempdata_year_temp_station = tempdata[[\"Year\", \"Temperature\", \"Station\"]]\n",
    "tempdata_year_temp_station[\"Year\"] = pd.DatetimeIndex(tempdata['Year']).year \n",
    "\n",
    "\n",
    "tempdata_year_temp_station_max = tempdata_year_temp_station.sort_values(\"Temperature\" , ascending=False)\n",
    "\n",
    "n = 10 # print the first 10 rows\n",
    "print(\"This are the first {} of the max temperature: \".format(n))\n",
    "tempdata_year_temp_station_max.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This are the first 10 of the max temperature: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Station</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>1956</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>1956</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8288</th>\n",
       "      <td>1960</td>\n",
       "      <td>-28.3</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5434</th>\n",
       "      <td>1958</td>\n",
       "      <td>-27.9</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>1956</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>1956</td>\n",
       "      <td>-26.2</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>1955</td>\n",
       "      <td>-26.2</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>1958</td>\n",
       "      <td>-25.6</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277</th>\n",
       "      <td>1955</td>\n",
       "      <td>-25.6</td>\n",
       "      <td>102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>2016</td>\n",
       "      <td>-25.5</td>\n",
       "      <td>102170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Temperature  Station\n",
       "2432  1956        -30.0   102190\n",
       "2431  1956        -29.0   102190\n",
       "8288  1960        -28.3   102190\n",
       "5434  1958        -27.9   102190\n",
       "2468  1956        -27.0   102190\n",
       "2436  1956        -26.2   102190\n",
       "2279  1955        -26.2   102190\n",
       "5426  1958        -25.6   102190\n",
       "2277  1955        -25.6   102190\n",
       "1618  2016        -25.5   102170"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdata_year_temp_station_min = tempdata_year_temp_station.sort_values(\"Temperature\" , ascending=True)\n",
    "\n",
    "n = 10 # print the first 10 rows\n",
    "print(\"This are the first {} of the max temperature: \".format(n))\n",
    "tempdata_year_temp_station_min.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment to 1b):\n",
    "- This part (1.b) needs to be saved as .py! \n",
    "- Run script + check run time \n",
    "- Compare run time with part 1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "- Count the number of readings for each month in the period of 1950-2014 which are higher than 10 degrees. Repeat the exercise, this time taking only distinct readings from each station. That is, if a station reported a reading above 10 degrees in some month, then it appears only once in the count for that month.\n",
    "In this exercise you will use the temperature-readings.csv file. The output should contain the following information:\n",
    "Year, month, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('1955', '09'), 55),\n",
       " (('1955', '10'), 23),\n",
       " (('1956', '03'), 4),\n",
       " (('1956', '04'), 7),\n",
       " (('1956', '05'), 60),\n",
       " (('1956', '06'), 92),\n",
       " (('1956', '07'), 108),\n",
       " (('1956', '08'), 84),\n",
       " (('1956', '09'), 54),\n",
       " (('1956', '10'), 17),\n",
       " (('1957', '03'), 1),\n",
       " (('1957', '04'), 13),\n",
       " (('1957', '05'), 46),\n",
       " (('1957', '06'), 72),\n",
       " (('1957', '07'), 109),\n",
       " (('1957', '08'), 96),\n",
       " (('1957', '09'), 43),\n",
       " (('1957', '10'), 9),\n",
       " (('1958', '04'), 4),\n",
       " (('1958', '05'), 40)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final code for exercise 2\n",
    "y = lines.map(lambda x: ( (x[1][0:4],x[1][5:7]),float(x[3]) ))\n",
    "\n",
    "y = y.filter(lambda x: int(x[0][0]) >= 1950 and int(x[0][0]) <= 2014)\n",
    "\n",
    "#############################\n",
    "tempsOver= y.filter(lambda x : x[1] >= 10)\n",
    "\n",
    "tempsOver_unpack=tempsOver.map(lambda x: ( (x[0][0], x[0][1]),1))\n",
    "\n",
    "tempsOver10=tempsOver_unpack.reduceByKey(lambda x,y : x+y).sortByKey()\n",
    "tempsOver10.take(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2013', '11'), 1),\n",
       " (('2014', '04'), 1),\n",
       " (('2014', '08'), 1),\n",
       " (('2014', '11'), 1),\n",
       " (('1955', '09'), 1),\n",
       " (('1955', '10'), 1),\n",
       " (('1956', '04'), 1),\n",
       " (('1956', '08'), 1),\n",
       " (('1957', '04'), 1),\n",
       " (('1957', '08'), 1)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempsOver_unpack1=tempsOver_unpack.distinct()\n",
    "tempsOver_unpack1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Just play arounds __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2013', '11', 10.2, 1),\n",
       " ('2014', '04', 12.1, 1),\n",
       " ('2014', '04', 13.1, 1),\n",
       " ('2014', '04', 14.7, 1),\n",
       " ('2014', '04', 15.8, 1),\n",
       " ('2014', '04', 11.8, 1),\n",
       " ('2014', '04', 13.3, 1),\n",
       " ('2014', '04', 15.5, 1),\n",
       " ('2014', '04', 16.6, 1),\n",
       " ('2014', '04', 17.3, 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list with year month and temperature + 1\n",
    "year_temperature = lines.map(lambda x: (x[1][0:4], x[1][5:7], float(x[3]), 1))\n",
    "year_temperature_timeinterval = year_temperature.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014 and x[2] > 10)\n",
    "#year_temperature_timeinterval_temp = year_temperature_timeinterval.filter(lambda x: int(x[0][1][1]) >10)\n",
    "year_temperature_timeinterval.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2013, 11, 1),\n",
       " (2014, 4, 1),\n",
       " (2014, 4, 1),\n",
       " (2014, 4, 1),\n",
       " (2014, 4, 1),\n",
       " (2014, 4, 1),\n",
       " (2014, 4, 1),\n",
       " (2014, 4, 1),\n",
       " (2014, 4, 1),\n",
       " (2014, 4, 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test just with year and month + 1\n",
    "year_temperature_test = lines.map(lambda x: (x[1][0:4], x[1][5:7], float(x[3]), 1))\n",
    "year_temperature_timeinterval_test = year_temperature_test.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014 and x[2] > 10)\n",
    "year_temperature_timeinterval_test = year_temperature_timeinterval_test.map(lambda x: (int(x[0]), int(x[1]), 1))\n",
    "year_temperature_timeinterval_test.take(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2013, 11), 1),\n",
       " ((2014, 4), 1),\n",
       " ((2014, 4), 1),\n",
       " ((2014, 4), 1),\n",
       " ((2014, 4), 1),\n",
       " ((2014, 4), 1),\n",
       " ((2014, 4), 1),\n",
       " ((2014, 4), 1),\n",
       " ((2014, 4), 1),\n",
       " ((2014, 4), 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test just with year and month + 1\n",
    "year_temperature_test = lines.map(lambda x: (x[1][0:4], x[1][5:7], float(x[3]), 1))\n",
    "year_temperature_timeinterval_test = year_temperature_test.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014 and x[2] > 10)\n",
    "year_temperature_timeinterval_test = year_temperature_timeinterval_test.map(lambda x: ((int(x[0]), int(x[1])), 1))\n",
    "year_temperature_timeinterval_test.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1955, 9), 55),\n",
       " ((1956, 4), 5),\n",
       " ((1956, 6), 88),\n",
       " ((1956, 8), 79),\n",
       " ((1956, 10), 16),\n",
       " ((1957, 3), 1),\n",
       " ((1957, 5), 43),\n",
       " ((1957, 7), 109),\n",
       " ((1957, 9), 41),\n",
       " ((1958, 4), 4),\n",
       " ((1958, 6), 89),\n",
       " ((1958, 8), 96),\n",
       " ((1958, 10), 31),\n",
       " ((1959, 5), 57),\n",
       " ((1959, 7), 111),\n",
       " ((1959, 9), 56),\n",
       " ((1960, 4), 9),\n",
       " ((1960, 6), 108),\n",
       " ((1960, 8), 98),\n",
       " ((1960, 10), 2),\n",
       " ((1961, 3), 6),\n",
       " ((1961, 5), 39),\n",
       " ((2013, 11), 1),\n",
       " ((2014, 4), 9),\n",
       " ((2014, 6), 54),\n",
       " ((2014, 8), 56),\n",
       " ((2014, 10), 17),\n",
       " ((1955, 10), 22),\n",
       " ((1956, 3), 4),\n",
       " ((1956, 5), 56),\n",
       " ((1956, 7), 107),\n",
       " ((1956, 9), 51),\n",
       " ((1957, 4), 13),\n",
       " ((1957, 6), 72),\n",
       " ((1957, 8), 96),\n",
       " ((1957, 10), 9),\n",
       " ((1958, 5), 39),\n",
       " ((1958, 7), 110),\n",
       " ((1958, 9), 57),\n",
       " ((1959, 4), 9),\n",
       " ((1959, 6), 94),\n",
       " ((1959, 8), 101),\n",
       " ((1959, 10), 9),\n",
       " ((1960, 3), 1),\n",
       " ((1960, 5), 65),\n",
       " ((1960, 7), 110),\n",
       " ((1960, 9), 56),\n",
       " ((1961, 4), 20),\n",
       " ((2014, 5), 31),\n",
       " ((2014, 7), 62),\n",
       " ((2014, 9), 25),\n",
       " ((2014, 11), 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_temperature_timeinterval_test = year_temperature_timeinterval_test.sortByKey()\n",
    "year_temperature_timeinterval_test.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 71.0 failed 1 times, most recent failure: Lost task 0.0 in stage 71.0 (TID 75, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 370, in func\n    return f(iterator)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1876, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 239, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() takes 1 positional argument but 2 were given\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 370, in func\n    return f(iterator)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1876, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 239, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() takes 1 positional argument but 2 were given\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6be7e4c82399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0myear_temperature_timeinterval_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myear_temperature_timeinterval_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0myear_temperature_timeinterval_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 71.0 failed 1 times, most recent failure: Lost task 0.0 in stage 71.0 (TID 75, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 370, in func\n    return f(iterator)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1876, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 239, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() takes 1 positional argument but 2 were given\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 370, in func\n    return f(iterator)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1876, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 239, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() takes 1 positional argument but 2 were given\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "year_temperature_timeinterval_test = year_temperature_timeinterval_test.reduceByKey(lambda y: x[1]+y[1])\n",
    "year_temperature_timeinterval_test.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list with year and temperature + add 1 to every row\n",
    "year_month_temperature = lines.map(lambda x: (x[1][0:4], (x[1][5:7], x[3], 1))) # 1 - because, later in the exercise we want to count\n",
    "# code with degrees - year_month_temperature = lines.map(lambda x: (x[1][0:4], (x[1][5:7], (float(x[3]), 1)))) # 1 - because, later in the exercise we want to count - do not need this\n",
    "#year_month_temperature = year_month_temperature.filter(lambda x: x[1][1] > 10)\n",
    "year_month_temperature = year_month_temperature.filter(lambda x: int(x[0][0]) >= 1950 and int(x[0][0]) <= 2014) # filter by year\n",
    "#year_month_temperature = year_month_temperature.filter(lambda x: int(x[1][1]) > 10)\n",
    "year_month_temperature.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('11', 1),\n",
       " ('11', 1),\n",
       " ('11', 1),\n",
       " ('11', 1),\n",
       " ('11', 1),\n",
       " ('11', 1),\n",
       " ('11', 1),\n",
       " ('11', 1),\n",
       " ('11', 1),\n",
       " ('11', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with year 2013\n",
    "year_month_temperature_test = lines.map(lambda x: (x[1][5:7], 1))\n",
    "year_month_temperature.filter(lambda x: x[0] == \"2013\")\n",
    "year_month_temperature_test.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12', 895),\n",
       " ('03', 891),\n",
       " ('05', 880),\n",
       " ('06', 773),\n",
       " ('07', 736),\n",
       " ('09', 809),\n",
       " ('10', 836),\n",
       " ('11', 867),\n",
       " ('01', 896),\n",
       " ('02', 820),\n",
       " ('04', 861),\n",
       " ('08', 736)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_month_temperature_test.reduceByKey(lambda x,y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2013',\n",
       "  '1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212'),\n",
       " ('2014',\n",
       "  '01010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030304040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040405050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080809090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090910101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212'),\n",
       " ('2015',\n",
       "  '01010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030304040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040405050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080809090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090910101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212'),\n",
       " ('2016',\n",
       "  '0101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050506060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060607'),\n",
       " ('1956',\n",
       "  '01010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050506060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909091010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212'),\n",
       " ('1957',\n",
       "  '0101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010102020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050506060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060607070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070708080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212'),\n",
       " ('1955',\n",
       "  '090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212'),\n",
       " ('1958',\n",
       "  '0101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010102020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060607070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909091010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212'),\n",
       " ('1959',\n",
       "  '01010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020203030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030304040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040405050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080809090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090910101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212'),\n",
       " ('1960',\n",
       "  '0101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050506060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606060606070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070707070808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080808080909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212121212'),\n",
       " ('1961',\n",
       "  '0101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020202020303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030303030404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040404040505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505050505')]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_month_temperature_test2 = lines.map(lambda x: (x[1][0:4], (x[1][5:7])))\n",
    "year_month_temperature_test2.reduceByKey(lambda x,y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2013', ('11', 1))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_month_temperature.take(10)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter by the year >=1950 & <= 2014 + temperature > 10\n",
    "year_month_temperature_min10_timeinterval = year_month_temperature.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014 and x[2]> 10)\n",
    "year_month_temperature_min10_timeinterval.take(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3\n",
    "- Find the average monthly temperature for each available station in Sweden. Your result should include average temperature for each station for each month in the period of 1960- 2014. Bear in mind that not every station has the readings for each month in this timeframe.\n",
    "\n",
    "- In this exercise you will use the *temperature-readings.csv* file.\n",
    "\n",
    "- The output should contain the following information: Year, month, station number, average monthly temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2013', '11', '01', '102170'), 6.8),\n",
       " (('2013', '11', '01', '102170'), 3.8),\n",
       " (('2013', '11', '02', '102170'), 5.8),\n",
       " (('2013', '11', '02', '102170'), -1.1),\n",
       " (('2013', '11', '03', '102170'), -0.2),\n",
       " (('2013', '11', '03', '102170'), 5.6),\n",
       " (('2013', '11', '04', '102170'), 6.5),\n",
       " (('2013', '11', '04', '102170'), 5.1),\n",
       " (('2013', '11', '05', '102170'), 4.2),\n",
       " (('2013', '11', '05', '102170'), 3.2)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = lines.map(lambda x: ( (x[1][0:4],x[1][5:7],x[1][8:10],x[0]),float(x[3]) ))\n",
    "\n",
    "rdd = rdd.filter(lambda x: int(x[0][0]) >= 1960 and int(x[0][0]) <= 2014)\n",
    "\n",
    "#############################\n",
    "\n",
    "rdd=rdd.map(lambda x: ( (x[0][0],x[0][1],x[0][2], x[0][3]),float(x[1]) ) )\n",
    "\n",
    "rdd.take(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2014', '12', '31', '102170'), -7.05),\n",
       " (('2014', '12', '30', '102170'), -9.600000000000001),\n",
       " (('2014', '12', '29', '102170'), -12.8),\n",
       " (('2014', '12', '28', '102170'), -20.200000000000003),\n",
       " (('2014', '12', '27', '102170'), -21.05),\n",
       " (('2014', '12', '26', '102170'), -19.9),\n",
       " (('2014', '12', '25', '102170'), -18.65),\n",
       " (('2014', '12', '24', '102170'), -12.8),\n",
       " (('2014', '12', '23', '102170'), -11.850000000000001),\n",
       " (('2014', '12', '22', '102170'), -6.0),\n",
       " (('2014', '12', '21', '102170'), -10.0),\n",
       " (('2014', '12', '20', '102170'), -7.050000000000001),\n",
       " (('2014', '12', '19', '102170'), 0.20000000000000007),\n",
       " (('2014', '12', '18', '102170'), -3.2),\n",
       " (('2014', '12', '17', '102170'), -6.75),\n",
       " (('2014', '12', '16', '102170'), -0.45),\n",
       " (('2014', '12', '15', '102170'), 2.8),\n",
       " (('2014', '12', '14', '102170'), -1.2000000000000002),\n",
       " (('2014', '12', '13', '102170'), -1.85),\n",
       " (('2014', '12', '12', '102170'), 1.7000000000000002),\n",
       " (('2014', '12', '11', '102170'), 1.5),\n",
       " (('2014', '12', '10', '102170'), 1.75),\n",
       " (('2014', '12', '09', '102170'), -7.25),\n",
       " (('2014', '12', '08', '102170'), -2.9499999999999997),\n",
       " (('2014', '12', '07', '102170'), 3.8),\n",
       " (('2014', '12', '06', '102170'), -3.2),\n",
       " (('2014', '12', '05', '102170'), -0.35),\n",
       " (('2014', '12', '04', '102170'), -3.3),\n",
       " (('2014', '12', '03', '102170'), -2.7),\n",
       " (('2014', '12', '02', '102170'), -0.35000000000000003),\n",
       " (('2014', '12', '01', '102170'), -1.1)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanRDD = (rdd\n",
    "           .mapValues(lambda x: (x, 1))\n",
    "           .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "           .mapValues(lambda x: x[0]/x[1]))\n",
    "meanRDD.sortByKey(ascending=False).take(31)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2014', '12', '102170'), -5.801612903225807),\n",
       " (('2014', '11', '102170'), 2.525),\n",
       " (('2014', '10', '102170'), 7.106451612903226),\n",
       " (('2014', '09', '102170'), 8.585),\n",
       " (('2014', '08', '102170'), 13.869354838709675),\n",
       " (('2014', '07', '102170'), 19.659677419354836),\n",
       " (('2014', '06', '102170'), 14.443333333333333),\n",
       " (('2014', '05', '102170'), 10.756451612903227),\n",
       " (('2014', '04', '102170'), 4.776666666666667),\n",
       " (('2014', '03', '102170'), 1.896774193548387),\n",
       " (('2014', '02', '102170'), 0.6749999999999999),\n",
       " (('2014', '01', '102170'), -4.106451612903226),\n",
       " (('2013', '12', '102170'), 0.7096774193548387),\n",
       " (('2013', '11', '102170'), -0.05166666666666663),\n",
       " (('1961', '05', '102190'), 10.61533333333333),\n",
       " (('1961', '04', '102190'), 4.507777777777778),\n",
       " (('1961', '03', '102190'), 2.708602150537634),\n",
       " (('1961', '02', '102190'), -2.1333333333333333),\n",
       " (('1961', '01', '102190'), -7.215053763440859),\n",
       " (('1960', '12', '102190'), -2.427419354838709),\n",
       " (('1960', '11', '102190'), -0.42999999999999994),\n",
       " (('1960', '10', '102190'), 2.0943548387096773),\n",
       " (('1960', '09', '102190'), 9.402222222222223),\n",
       " (('1960', '08', '102190'), 13.41989247311828),\n",
       " (('1960', '07', '102190'), 14.205645161290322),\n",
       " (('1960', '06', '102190'), 15.769722222222223),\n",
       " (('1960', '05', '102190'), 10.768548387096773),\n",
       " (('1960', '04', '102190'), 2.5100000000000002),\n",
       " (('1960', '03', '102190'), -2.2798387096774193),\n",
       " (('1960', '02', '102190'), -9.61810344827586),\n",
       " (('1960', '01', '102190'), -6.875)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanRDD.map(lambda x: ( (x[0][0],x[0][1],x[0][3]),x[1]) )\\\n",
    ".mapValues(lambda x: (x, 1))\\\n",
    ".reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\\\n",
    ".mapValues(lambda x: x[0]/x[1]).sortByKey(ascending=False).take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4\n",
    "- Provide a list of stations with their associated maximum measured temperatures and maximum measured daily precipitation. Show only those stations where the maximum temperature is between 25 and 30 degrees and maximum daily precipitation is between 100 mm and 200mm.\n",
    "\n",
    "- In this exercise you will use the *temperature-readings.csv* and *precipitation-readings.csv* files.\n",
    "\n",
    "- The output should contain the following information: Station number, maximum measured temperature, maximum daily precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_A4 = \"/Users/phillipholscher/Desktop/precipitation-readings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('102170', '2014-07-22'), 28.0),\n",
       " (('102170', '2014-07-23'), 29.1),\n",
       " (('102170', '2014-07-26'), 28.7),\n",
       " (('102170', '2014-07-29'), 25.5),\n",
       " (('102170', '2015-07-01'), 26.8)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp  = sc.textFile(path)\n",
    "l = temp.map(lambda line: line.split(\";\"))\n",
    "\n",
    "station_temperature = l.map(lambda x:( (x[0],x[1]) ,float(x[3])))\n",
    "\n",
    "#station_temperature = station_temperature.filter(lambda x: int(x[0]) >= 1950 and int(x[0]) <= 2014)\n",
    "\n",
    "max_temp_station= station_temperature.reduceByKey(lambda x,y :x if x>=y else y)\n",
    "\n",
    "max_temp_station=max_temp_station.filter(lambda x: x[1]>=25. and x[1]<=30.)\n",
    "\n",
    "max_temp_st=max_temp_station.map(lambda x: (x[0],float(x[1])))\n",
    "max_temp_st.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('105220', '1997-07-01'), 18.7),\n",
       " (('105220', '2004-02-04'), 10.0),\n",
       " (('106570', '1997-06-10'), 12.8),\n",
       " (('106570', '2004-07-19'), 15.5),\n",
       " (('108320', '2010-06-30'), 10.0),\n",
       " (('114140', '2009-07-30'), 10.7),\n",
       " (('114140', '2012-08-26'), 11.5),\n",
       " (('114410', '2013-07-31'), 12.2),\n",
       " (('122260', '2010-08-03'), 10.1),\n",
       " (('123060', '1996-06-17'), 11.9)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep=sc.textFile(path_A4)\n",
    "\n",
    "L=prep.map(lambda line: line.split(\";\"))\n",
    "\n",
    "st = L.map(lambda x: ( (x[0],x[1]) ,float(x[3])) )\n",
    "\n",
    "max_st= st.reduceByKey(lambda x,y :x if x>=y else y)\n",
    "\n",
    "max_st=max_st.filter(lambda x: x[1]>=10. and x[1]<=20.)\n",
    "\n",
    "wst=max_st.map(lambda x: (x[0],float(x[1])))\n",
    "\n",
    "wst.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_temp_st.join(wst).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 5 \n",
    "- Calculate the average monthly precipitation for the Östergotland region (list of stations is provided in the separate file) for the period 1993-2016. In order to do this, you will first need to calculate the total monthly precipitation for each station before calculating the monthly average (by averaging over stations).\n",
    "\n",
    "- In this exercise you will use the *precipitation-readings.csv* and *stations-Ostergotland.csv* files. HINT (not for the SparkSQL lab): Avoid using joins here! stations-Ostergotland.csv is small and if distributed will cause a number of unnecessary shuffles when joined with precipitationRDD. If you distribute precipitation-readings.csv then either repartition your stations RDD to 1 partition or make use of the collect to acquire a python list and broadcast function to broadcast the list to all nodes.\n",
    "\n",
    "- The output should contain the following information: Year, month, average monthly precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_A5stations = \"/Users/phillipholscher/Desktop/stations-Ostergotland.csv\"\n",
    "path_A5precipitation = \"/Users/phillipholscher/Desktop/precipitation-readings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations=sc.textFile(path_A5stations)\n",
    "precipitation=sc.textFile(path_A5precipitation)\n",
    "\n",
    "stations=stations.map(lambda line: line.split(\";\"))\n",
    "precipitation=precipitation.map(lambda line:line.split(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('1995', '08', '01', '103100'), 0.0),\n",
       " (('1995', '08', '01', '103100'), 0.0),\n",
       " (('1995', '08', '01', '103100'), 0.0),\n",
       " (('1995', '08', '01', '103100'), 0.0),\n",
       " (('1995', '08', '01', '103100'), 0.0),\n",
       " (('1995', '08', '01', '103100'), 0.0),\n",
       " (('1995', '08', '01', '103100'), 0.0),\n",
       " (('1995', '08', '01', '103100'), 0.0),\n",
       " (('1995', '08', '01', '103100'), 0.0),\n",
       " (('1995', '08', '01', '103100'), 0.0)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD = precipitation.map(lambda x: ( (x[1][0:4],x[1][5:7],x[1][8:10],x[0]),float(x[3]) ))\n",
    "\n",
    "RDD = RDD.filter(lambda x: int(x[0][0]) >= 1993 and int(x[0][0]) <= 2016)\n",
    "\n",
    "#############################\n",
    "\n",
    "#RDD_filtered=RDD.map(lambda x: (x[0][0],x[0][1], x[0][2],x[0][3],float(x[1]) ))\n",
    "\n",
    "RDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2016', '07', '01', '99280'), 0.014285714285714287),\n",
       " (('2016', '07', '01', '98490'), 0.09999999999999999),\n",
       " (('2016', '07', '01', '98040'), 0.0),\n",
       " (('2016', '07', '01', '97530'), 0.62),\n",
       " (('2016', '07', '01', '97510'), 0.26),\n",
       " (('2016', '07', '01', '97370'), 0.02),\n",
       " (('2016', '07', '01', '97100'), 0.34285714285714286),\n",
       " (('2016', '07', '01', '96560'), 0.24285714285714288),\n",
       " (('2016', '07', '01', '96190'), 0.0),\n",
       " (('2016', '07', '01', '95540'), 0.0)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mRDD = (RDD\n",
    "           .mapValues(lambda x: (x, 1))\n",
    "           .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "           .mapValues(lambda x: x[0]/x[1]))\n",
    "mRDD.sortByKey(ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('1993', '01', '154860'), 0.1295698924731183),\n",
       " (('1993', '01', '188790'), 0.017633333333333338),\n",
       " (('1993', '01', '97510'), 0.030277777777777775),\n",
       " (('1993', '02', '154860'), 0.033384387351778656),\n",
       " (('1993', '02', '188790'), 0.036128364389233954),\n",
       " (('1993', '02', '97510'), 0.02008281573498965),\n",
       " (('1993', '03', '154860'), 0.028341013824884784),\n",
       " (('1993', '03', '188790'), 0.07755921770297648),\n",
       " (('1993', '03', '97510'), 0.026113671274961593),\n",
       " (('1993', '04', '154860'), 0.006488957902001381)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M=mRDD.map(lambda x: ( (x[0][0],x[0][1],x[0][3]),x[1]) )\\\n",
    ".mapValues(lambda x: (x, 1))\\\n",
    ".reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\\\n",
    ".mapValues(lambda x: x[0]/x[1]).sortByKey()\n",
    "M.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['84260',\n",
       " '85630',\n",
       " '85460',\n",
       " '85450',\n",
       " '85490',\n",
       " '85650',\n",
       " '85390',\n",
       " '85410',\n",
       " '86470',\n",
       " '86440',\n",
       " '86420',\n",
       " '86340',\n",
       " '86350',\n",
       " '86360',\n",
       " '86370',\n",
       " '86200',\n",
       " '86330',\n",
       " '87140',\n",
       " '87150',\n",
       " '86130',\n",
       " '86090',\n",
       " '85280',\n",
       " '85250',\n",
       " '85240',\n",
       " '85270',\n",
       " '85220',\n",
       " '85210',\n",
       " '85180',\n",
       " '85040',\n",
       " '85050',\n",
       " '75520',\n",
       " '85160',\n",
       " '85600',\n",
       " '85130']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_ost=sc.textFile(path_A5stations)\n",
    "\n",
    "stations_ost=stations_ost.map(lambda line: line.split(\";\"))\n",
    " \n",
    "stations_ost=stations_ost.map(lambda x : x[0])\n",
    "stations_ost.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=sc.broadcast(stations_ost.collect()).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('1993', '04', '86340'), 0.0),\n",
       " (('1993', '05', '86340'), 0.028360215053763436),\n",
       " (('1993', '06', '86340'), 0.08851506786289397),\n",
       " (('1993', '07', '86340'), 0.12822580645161288),\n",
       " (('1993', '08', '86340'), 0.13591977466977462),\n",
       " (('1993', '09', '86340'), 0.05759920634920635),\n",
       " (('1993', '10', '86340'), 0.05815217391304348),\n",
       " (('1993', '11', '86340'), 0.05952020202020202),\n",
       " (('1993', '12', '86340'), 0.04989481065918653),\n",
       " (('1994', '01', '86340'), 0.0407169862273832),\n",
       " (('1994', '02', '86340'), 0.033709180312441175),\n",
       " (('1994', '03', '86340'), 0.05174230281172778),\n",
       " (('1994', '04', '86340'), 0.032956349206349204),\n",
       " (('1994', '05', '86340'), 0.03373655913978495),\n",
       " (('1994', '06', '86340'), 0.06393115942028986),\n",
       " (('1994', '07', '86340'), 0.0),\n",
       " (('1994', '08', '86340'), 0.08052155496054514),\n",
       " (('1994', '09', '86340'), 0.1336098852202971),\n",
       " (('1994', '10', '86340'), 0.044763908368396436),\n",
       " (('1994', '11', '86340'), 0.021805555555555557),\n",
       " (('1994', '12', '86340'), 0.06851332398316971),\n",
       " (('1995', '01', '86340'), 0.037009116409537156),\n",
       " (('1995', '02', '86340'), 0.04776432806324111),\n",
       " (('1995', '03', '86340'), 0.04652040339277365),\n",
       " (('1995', '04', '86340'), 0.08670893719806765),\n",
       " (('1995', '05', '86340'), 0.03780037400654511),\n",
       " (('1995', '06', '86340'), 0.13706529581529578),\n",
       " (('1995', '07', '86340'), 0.05860215053763441),\n",
       " (('1995', '08', '85460'), 0.03279569892473118),\n",
       " (('1995', '08', '86340'), 0.010612435717625058),\n",
       " (('1995', '09', '85460'), 0.23342000568343285),\n",
       " (('1995', '09', '86340'), 0.145518115942029),\n",
       " (('1995', '10', '85050'), 0.025672043010752685),\n",
       " (('1995', '10', '85460'), 0.05120884258331663),\n",
       " (('1995', '10', '86340'), 0.00913978494623656),\n",
       " (('1995', '11', '85050'), 0.06375007842399147),\n",
       " (('1995', '11', '85460'), 0.05710091382879988),\n",
       " (('1995', '11', '86340'), 0.11606481481481484),\n",
       " (('1995', '12', '75520'), 0.016314243088539765),\n",
       " (('1995', '12', '85050'), 0.00563973063973064),\n",
       " (('1995', '12', '85460'), 0.01757348900535584),\n",
       " (('1995', '12', '86340'), 0.0046014492753623185),\n",
       " (('1995', '12', '86420'), 0.011331667054173447),\n",
       " (('1995', '12', '87140'), 0.007992327365728899),\n",
       " (('1996', '01', '75520'), 0.013546131288066773),\n",
       " (('1996', '01', '85050'), 0.014544627555619711),\n",
       " (('1996', '01', '85460'), 0.01898128566560692),\n",
       " (('1996', '01', '86340'), 0.010992286115007015),\n",
       " (('1996', '01', '86420'), 0.02271464646464646),\n",
       " (('1996', '01', '87140'), 0.009158379021632877),\n",
       " (('1996', '02', '75520'), 0.018494324266438208),\n",
       " (('1996', '02', '85050'), 0.02464616176760105),\n",
       " (('1996', '02', '85460'), 0.018512237387799606),\n",
       " (('1996', '02', '86340'), 0.03132183908045977),\n",
       " (('1996', '02', '86420'), 0.03922240369502033),\n",
       " (('1996', '02', '87140'), 0.013622166189632457),\n",
       " (('1996', '03', '75520'), 0.00793010752688172),\n",
       " (('1996', '03', '85050'), 0.008709677419354838),\n",
       " (('1996', '03', '85460'), 0.022747222778779583),\n",
       " (('1996', '03', '86340'), 0.014516129032258063),\n",
       " (('1996', '03', '86420'), 0.023184261974584553),\n",
       " (('1996', '03', '87140'), 0.006121700879765397),\n",
       " (('1996', '04', '75520'), 0.003309814391050089),\n",
       " (('1996', '04', '85050'), 0.009694444444444445),\n",
       " (('1996', '04', '85460'), 0.021465744400527006),\n",
       " (('1996', '04', '86340'), 0.01728030303030303),\n",
       " (('1996', '04', '86420'), 0.017457729468599036),\n",
       " (('1996', '04', '87140'), 0.011323099415204678),\n",
       " (('1996', '05', '75520'), 0.09683263207106124),\n",
       " (('1996', '05', '85050'), 0.0858637213651239),\n",
       " (('1996', '05', '85460'), 0.10594323614948975),\n",
       " (('1996', '05', '86340'), 0.07348112966976922),\n",
       " (('1996', '05', '86420'), 0.11164801187592212),\n",
       " (('1996', '05', '87140'), 0.05142589995324917),\n",
       " (('1996', '06', '75520'), 0.08876811594202899),\n",
       " (('1996', '06', '85050'), 0.07522946859903382),\n",
       " (('1996', '06', '85460'), 0.07502817581393097),\n",
       " (('1996', '06', '86340'), 0.06694687558817994),\n",
       " (('1996', '06', '86420'), 0.08788537549407115),\n",
       " (('1996', '06', '87140'), 0.058894927536231885),\n",
       " (('1996', '07', '75520'), 0.13206605222734258),\n",
       " (('1996', '07', '85050'), 0.13393418759828293),\n",
       " (('1996', '07', '85460'), 0.08947086752589599),\n",
       " (('1996', '07', '86340'), 0.08960599973093512),\n",
       " (('1996', '07', '86420'), 0.13206020837507515),\n",
       " (('1996', '07', '87140'), 0.12916666666666665),\n",
       " (('1996', '08', '75520'), 0.08128054740957966),\n",
       " (('1996', '08', '85050'), 0.03508064516129032),\n",
       " (('1996', '08', '85460'), 0.0452753768585687),\n",
       " (('1996', '08', '86340'), 0.031780037400654514),\n",
       " (('1996', '08', '86420'), 0.07448671497584541),\n",
       " (('1996', '08', '87140'), 0.045652173913043485),\n",
       " (('1996', '09', '75520'), 0.049305555555555554),\n",
       " (('1996', '09', '85050'), 0.06167270531400966),\n",
       " (('1996', '09', '85460'), 0.08346583850931677),\n",
       " (('1996', '09', '86340'), 0.07571256038647343),\n",
       " (('1996', '09', '86420'), 0.13530248133509007),\n",
       " (('1996', '09', '87140'), 0.07723429951690822),\n",
       " (('1996', '10', '75520'), 0.02148989898989899),\n",
       " (('1996', '10', '85050'), 0.03323232323232323)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#M1.intersection(stations_ost).collect()\n",
    "M.filter(lambda x: x[0][2] in S).take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 6 \n",
    "- Compare the average monthly temperature (find the difference) in the period 1950-2014 for all stations in Östergotland with long-term monthly averages in the period of 1950-1980. Make a plot of your results.\n",
    "HINT: The first step is to find the monthly averages for each station. \n",
    "- Then, you can average over all stations to acquire the average temperature for a specific year and month. This RDD/Data Frame can be used to compute the long-term average by averaging over all the years in theinterval.\n",
    "\n",
    "- The output should contain the following information: Year, month, difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
